---
title: "Econometric Modeling"
author: "AECN 396/896-002"
output:
  xaringan::moon_reader:
    # css: [default, metropolis, metropolis-fonts] 
    css: xaringan-themer.css 
    self_contained: true
    nature:
      ratio: 4:3
      highlightStyle: github
      highlightSpans: true
      highlightLines: true
      countIncrementalSlides: false
---
class: middle
layout: true

---

```{r, child = './../setup.Rmd'}
```

```{r echo = F, eval = F}
setwd(here("LectureNotes/7_EconometricModel"))
```

```{r packages, include = F, cache = F}
#--- load packages ---#
library(broom)
library(fixest)
library(margins)
library(patchwork)
library(readstata13)
library(modelsummary)
library(car)
library(stargazer)
library(lmtest)
```

# Before we start

## Learning objectives

1. Enhance the understanding of the interpretation of various models
1. Post-estimation simulation

## Table of contents

1. [Expanding on Simple Models](#various-models)
2. [Interaction terms](#interaction)
3. [Categorical variable](#qualitative)
4. [R coding tips: categorical variables and interaction terms](#use-i)
5. [Other miscellaneous topics](#misc)

---
class: inverse, center, middle
name: various-models

# More on functional forms

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---

# Various econometric models

.content-box-red[**log-linear**]

$log(y_i)= \beta_0+\beta_1 x_i + u_i$ 

<br>

.content-box-red[**linear-log**]

$y_i= \beta_0+\beta_1 log(x_i) + u_i$ 

<br>

.content-box-red[**log-log**]

$log(y_i)= \beta_0+\beta_1 log(x_i) + u_i$ 

--

<br>

.content-box-red[**quadratic**]

$y_i= \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + u_i$ 

---

# Quadratic


.content-box-red[**Model**]

$y_i= \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + u_i$ 

--

<br>

.content-box-red[**Calculus**]

Differentiating the both sides wrt $x_i$,

$\frac{\partial y_i}{\partial x_i} = \beta_1 + 2*\beta_2 x_i\Rightarrow  \Delta y_i = (\beta_1 + 2*\beta_2 x_i)\Delta x_i$

<br>

.content-box-red[**Interpretation**]

When $x$ increases by 1 unit $(\Delta x_i=1)$, $y$ increases by $\beta_1 + 2*\beta_2 x_i$

---

# Visualization

Quadratic functional form is quite flexible.

```{r echo = F}
N <- 1000
x <- seq(0, 2, length = N)
y_1 <- x + x^2
y_2 <- 3 * x - 2 * x^2
y_1_data <- data.table(
  y = y_1,
  x = x
)
y_2_data <- data.table(
  y = y_2,
  x = x
)

g_quad_1 <- ggplot(data = y_1_data) +
  geom_line(aes(y = y, x = x), color = "red")

g_quad_2 <- ggplot(data = y_2_data) +
  geom_line(aes(y = y, x = x), color = "red")
```

.left5[

$y = x + x^2$ $(\beta_1 = 1, \beta_2 = 1)$

```{r echo = F, out.width = "90%", out.height = "80%"}
g_quad_1
```
]

.right5[

$y = 3x-2x^2$ $(\beta_1 = 3, \beta_2 = -2)$

```{r echo = F, out.width = "90%", out.height = "80%"}
g_quad_2
```
]

---

# Example

.content-box-red[**Education impacts of income**]

The marginal impact of education (the impact of a small change in education on income) may differ what level of education you have had:

+ How much does it help to have two more years of education when you have had education until elementary school?

+ How much does it help to have two more years of education when you have graduated a college?

+ How much does it help to spend two more years as a Ph.D student if you have already spent six years in a Ph.D program

--

<br>

.content-box-green[**Observation**]

The marginal impact of education does not seem to be linear.

---

# Implementation in R

When you include a variable that is a transformation of an existing variable, use `I()` function in which you write the mathematical expression of the desired transformation.

```{r }
#--- prepare a dataset ---#
data("wage1", package = "wooldridge")

#--- run a regression ---#
quad_reg <- fixest::feols(wage ~ female + educ + I(educ^2), data = wage1)

#--- look at the results ---#
broom::tidy(quad_reg)
```

---

.content-box-red[**Estimated Model**]

$wage = 5.60 - 2.12\times female -0.416\times educ + 0.039\times educ^2$

```{r echo = F, out.width = "60%"}
N <- 1000
x <- seq(min(wage1$educ), max(wage1$educ), length = N)
data <- data.table(educ = x, female = 1) %>%
  mutate(y = predict(quad_reg, newdata = .))

g_quad_ex <- ggplot() +
  geom_point(data = wage1, aes(y = wage, x = educ), size = 0.4) +
  geom_line(data = data, aes(y = y, x = x))

g_quad_ex
```

---

.content-box-red[**Estimated Model**]

$wage = 5.60 - 2.12\times female -0.416\times educ + 0.039\times educ^2$

<br>

--

.content-box-green[**Problem**]

What is the marginal impact of $educ$?

$\frac{\partial wage}{\partial educ} = ?$

--

<br>

.content-box-green[**Answer**]

$\frac{\partial wage}{\partial educ} = -0.416+0.039\times 2\times educ$

--

+ When $educ = 4$, additional year of education is going to increase hourly wage by `r -0.416+0.078*4` on average 

--

+ When $educ = 10$, additional year of education is going to increase hourly wage by `r -0.416+0.078*10` on average

---

# Statistical significance of the marginal impact

The marginal impact of $educ$ is:

$\frac{\partial wage}{\partial educ} = -0.416+0.039\times 2\times educ$

+ $educ$: $-0.416$ $(t$-stat $= -1.80)$
+ $educ^2$: $0.039$ $(t$-stat $= 4.10)$

--

<br>

.content-box-green[**Question**]

So, is the marginal impact of $educ$ statistically significantly different from $0$?

---

# In the linear case

```{r }
linear_reg <- fixest::feols(wage ~ female + educ, data = wage1)
broom::tidy(linear_reg)
```

--

<br>

.content-box-red[**Estimated model**]

$wage = 0.62+0.51 \times educ$

---
class: middle

.content-box-red[**Estimated model**]

$wage = 0.62+0.51 \times educ$

--

<br>

.content-box-green[**Question**]

+ What is the marginal impact of $educ$?

--

0.51

--

+ Does the marginal impact of education vary depending on the level of education? 

--

No, the model we estimated assumed that the marginal impact of education is constant.

--

<br>

.content-box-red[**Testing**]

You can just test if $\hat{\beta}_{educ}$ (the marginal impact of education) is statistically significantly different from $0$, which is just a t-test.

---

# Going back to the quadratic case

With the quadratic specification  

+ The marginal impact of education varies depending on your education level

--

+ There is no single test that tells you whether the marginal impact of education is statistically significant universally

--

+ Indeed, you need different tests for different values education levels

---

# Example 1


.content-box-red[**Marginal impact of education**]

$\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times educ$

--

<br>

.content-box-red[**Hypothesis testing**]

Does additional year of education has a statistically significant impact (positive or negative) if your current education level is 4?    
+ $H_0$: $\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 4 =0$

+ $H_1$: $\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 4 \ne 0$

--

<br>

.content-box-green[**Question**]

Is this

+ test of a single coefficient? (t-test)
+ test of a single equation with multiple coefficients? (t-test)
+ test of multiples equations with multiple coefficients? (F-test)

---

.content-box-red[**t-statistic**]

$t = \frac{\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 4}{se(\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 4)} = \frac{\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 8}{se(\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 8)}$

--

<br>

.content-box-red[**R implementation**]

Remember, a trick to do this test using R is take advantage of the fact that $F_{1, n-k-1} \sim t_{n-k-1}^2$.

```{r }
car::linearHypothesis(quad_reg, "educ + 8*I(educ^2)=0")
```

--

Since the p-value is 0.529, we do not reject the null.

---
class: middle

# Example 2

.content-box-red[**Marginal impact of education**]

$\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times educ$

--

<br>

.content-box-red[**Hypothesis testing**]

Does additional year of education has a statistically significant impact (positive or negative) if your current education level is 4?    
+ $H_0$: $\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 10 =0$

+ $H_1$: $\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 10 \ne 0$

--

<br>

.content-box-green[**Question**]

Is this

+ test of a single coefficient? (t-test)
+ test of a single equation with multiple coefficients? (t-test)
+ test of multiples equations with multiple coefficients? (F-test)

---

.content-box-red[**t-statistic**]

$t = \frac{\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 10}{se(\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 10)} = \frac{\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 20}{se(\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 20)}$

--

<br>

.content-box-red[**R implementation**]

```{r }
car::linearHypothesis(quad_reg, "educ + 20*I(educ^2)=0")
```

--

Since the much lower than is 0.01, we can reject the null at the 1% level.

---
class: inverse, center, middle
name: interaction

# Interaction terms

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---
class: middle

.content-box-red[**An interaction term**]

A variable that is a multiplication of two variables 

--

<br>

.content-box-red[**Example**]

$educ\times exper$

---

<br>

.content-box-red[**A model with an interaction term**]

$wage = \beta_0 + \beta_1 exper + \beta_2 educ \times exper + u$ 

--

<br>

.content-box-red[**Marginal impact of experience**]

$\frac{\partial wage}{\partial exper} = \beta_1+\beta_2\times educ$ 

--

<br>

.content-box-red[**Implications**]
  
The marginal impact of experience depends on education

+ $\beta_1$: the marginal impact of experience when $educ=?$

+ if $\beta_2>0$: additional year of experience is worth more when you have more years of education

---

# Regression with interaction terms

Just like the quadratic case with $educ^2$, you can use `I()`.

```{r }
reg_int <- fixest::feols(wage ~ female + exper + I(exper * educ), data = wage1)
```

--

```{r echo = F}
modelsummary(reg_int, gof_omit = "R|IC|F|Log|Obs", stars = T)
```
  
---
class: middle

.content-box-red[**Estimated Model**]

$wage = 6.121 - 2.418 \times female - 0.188 \times exper + 0.020 \times educ \times exper$ 

<br>

.content-box-red[**Marginal impact of experience**]

$\frac{\partial wage}{\partial exper} = - 0.188 + 0.020 \times educ$   
---
class: middle

<br>
<br>

```{r echo = F}
N <- 1000
educ_temp <- seq(min(wage1$educ), max(wage1$educ), length = N)
min_exper <- reg_int$coefficients["exper"] + reg_int$coefficients["I(exper * educ)"] * educ_temp
data_exper <- data.table(
  x = educ_temp,
  y = min_exper
)

g_marginal <- ggplot(data = data_exper) +
  geom_line(aes(x = x, y = y)) +
  ylab("marginal impact of experience") +
  xlab("education") +
  geom_hline(yintercept = 0)

g_hist <- ggplot(data = wage1) +
  geom_histogram(aes(educ)) +
  xlab("education")
```

.left5[

Marginal impact of $exper$:

```{r echo = F, out.width="90%"}
g_marginal
```
]

.right5[

Histogram of education:

```{r echo = F, out.width="90%"}
g_hist
```
]

---
class: middle

.content-box-red[**Test of marginal impacts**]


+ Just like the case of the quadratic specification of education, marginal impact of experience is not constant

+ We can test if the marginal impact of experience is statistically significant for a given level of education

  * When $educ=10$, $\frac{\partial wage}{\partial exper} = - 0.188 + 0.020 \times 10=0.012$
  * When $educ=15$, $\frac{\partial wage}{\partial exper} = - 0.188 + 0.020 \times 15=0.112$

---
class: middle

.content-box-red[**Question**]

Does additional year of experience has a statistically significant impact (positive or negative) if your current education level is 10

<br>

.content-box-red[**Hypothesis**]
      
+ $H_0$: $\hat{\beta}_{exper} + \hat{\beta}_{exper\_educ} \times 10=0$

+ $H_1$: $\hat{\beta}_{exper} + \hat{\beta}_{exper\_educ} \times 10=0$

---
class: middle

.content-box-red[**R implementation**]

```{r }
car::linearHypothesis(reg_int, "exper+10*I(exper * educ)=0")
```

---

class: inverse, center, middle
name: qualitative

# Including qualitative information

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---

# Qualitative information

.content-box-green[**Issue**]

How do we include qualitative information as an independent variable?

--

<br>

.content-box-green[**Examples**]

+ male or female (binary)

+ married or single (binary)

+ high-school, college, masters, or Ph.D (more than two states)

---

# Binary variables


.content-box-red[**Dummy variable**]

+ Relevant information in binary variables can be captured by a .red[zero-one] variable that takes the value of $1$ for one state and $0$ for the other state

+ We use "dummy variable" to refer to a binary (zero-one) variable

<br>

.content-box-red[**Example**]

```{r }
dplyr::select(wage1, wage, educ, exper, female, married) %>%
  head()
```

---
class: middle

.content-box-red[**Model with dummy a variable**]

$wage = \beta_0 +\sigma_f female +\beta_2 educ + u$

<br>

.content-box-red[**Interpretation**]

+ `female`: $E[wage|female=1,educ] = \beta_0 + \sigma_f +\beta_2 educ$

+ `male`: $E[wage|female=0,educ] = \beta_0 + \beta_2 educ$

--

This means that 

$\sigma_f = E[wage|female=1,educ]-E[wage|female=0,educ]$

---
class: middle

$\sigma_f = E[wage|female=1,educ]-E[wage|female=0,educ]$

Verbally,

+ $\sigma_f$ is the difference in the expected wage conditional on education between female and male

+ $\sigma_f$ measures how much more (less) female workers make compared to male workers (.blue[baseline]) if they were to have the same education level 

---
class: middle


.content-box-red[**Regression with a dummy variable**]

```{r }
reg_df <- fixest::feols(wage ~ female + educ, data = wage1)

reg_df
```

<br>

.content-box-red[**Interpretation**]

Female workers make `r reg_df$coefficients["female"]` ($/hour) less than male workers on average even though they have the same education level. 

---


.content-box-red[**Visualization of the estimated model**]

```{r echo = F, fig.dim = c(6, 4)}
beta_0 <- reg_df$coefficients[1] # intercept
sigma_0 <- reg_df$coefficients["female"] # coef on female
beta_1 <- reg_df$coefficients["educ"] # coef on educ
x <- seq(0, 20, length = 1000)
fe_data <- data.table(x = x, y = beta_1 * x + beta_0 + sigma_0, type = "female")
ma_data <- data.table(x = x, y = beta_1 * x + beta_0, type = "male")
plot_data <- rbind(fe_data, ma_data)

text_data <- data.table(
  x = c(5, 12), y = c(6, 1),
  label = c(
    "wage == beta[0]+beta[2]*educ", "wage == beta[0]+sigma[f]+beta[2]*educ"
  )
)

g_comp <- ggplot(data = plot_data) +
  geom_line(aes(x = x, y = y, color = type)) +
  geom_text(
    data = text_data, aes(x, y, label = label),
    size = 6, parse = TRUE, family = "Times"
  ) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  ylab("Hourly Wage ($/hour)") +
  xlab("Education (years)") +
  scale_color_discrete(name = "") +
  theme(
    legend.position = "bottom",
    text = element_text(size = 16)
  )
g_comp
```

---
class: middle

.content-box-red[**Model with dummy a variable**]

$wage = \beta_0 +\sigma_m male +\beta_2 educ + u$

<br>

.content-box-red[**Interpretation**]

+ `male`: $E[wage|male = 1,educ] = \beta_0 + \sigma_m +\beta_2 educ$

+ `female`: $E[wage|male = 0,educ] = \beta_0 + \beta_2 educ$

--

This means that 

$\sigma_m = E[wage|male=1,educ]-E[wage|male=0,educ]$

---
class: middle

$\sigma_m = E[wage|male=1,educ]-E[wage|male=0,educ]$

Verbally,

+ $\sigma_m$ is the difference in the expected wage conditional on education between female and male

+ $\sigma_m$ measures how much more (less) male workers make compared to female workers (.blue[baseline]) if they were to have the same education level 

.red[Important]: whichever status that is given the value of $0$ becomes the baseline

---
class: middle


.content-box-red[**Regression with a dummy variable**]

```{r }
wage <- dplyr::mutate(wage, male = 1 - female)

reg_df <- fixest::feols(wage ~ male + educ, data = wage)

reg_df
```

<br>

.content-box-red[**Interpretation**]

Female workers make `r reg_df$coefficients["male"]` ($/hour) more than female workers on average even though they have the same education level. 

---

class: middle

.content-box-red[**Question**]

What do you think will happen if  we include both male and female dummy variables?   

--

<br>

.content-box-red[**Answer**]

+ They contain redundant information

+ Indeed, including both of them along with the intercept would cause .blue[perfect collinearity problem]

+ So, you .blue[need to] drop either one of them

--

<br>

.content-box-red[**Perfect Collinearity**]

intercept = male + female

---
class: middle

Here is what happens if you include both:

```{r highlight.output = c(8)}
reg_dmf <- fixest::feols(wage ~ male + female + educ, data = wage)

reg_dmf
```

---
class: middle

# Interactions with a dummy variable


.content-box-green[**Issue**]

+ In the previous example, the impact of education on wage was modeled to be exactly the same

+ Can we build a more flexible model that allows us to estimate the differential impacts of education on wage between male and female?

---
class: middle

.content-box-red[**A more flexible model**]

$wage = \beta_0 + \sigma_f female +\beta_2 educ + \gamma female\times educ + u$ 

+ [female]: $E[wage|female=1,educ] = \beta_0 + \sigma_f +(\beta_2+\gamma) educ$
+ [male]: $E[wage|female=0,educ] = \beta_0 + \beta_2 educ$

<br>

.content-box-red[**Interpretation**]

For female, education is more effective by $\gamma$ than it is for male.

---


.content-box-red[**Example using R**]

```{r eval = F}
reg_di <- fixest::feols(wage ~ female + educ + `I(female * educ)`, data = wage)
reg_di
```

```{r echo = F}
reg_di <- fixest::feols(wage ~ female + educ + I(female * educ), data = wage)
reg_di
```

--

<br>

.content-box-red[**Interpretation**]

The marginal benefit of education is 0.086 ($/hour) less for females workers than for male workers on average.

---

```{r echo = F, fig.dim = c(6, 4.5), out.width = "80%"}
beta_0 <- reg_di$coefficients[1] # intercept
sigma_0 <- reg_di$coefficients["female"] # coef on female
beta_1 <- reg_di$coefficients["educ"] # coef on educ
gamma <- reg_di$coefficients["I(female * educ)"]
x <- seq(0, 20, length = 1000)
fe_data <- data.table(x = x, y = (beta_1 + gamma) * x + beta_0 + sigma_0, type = "female")
ma_data <- data.table(x = x, y = beta_1 * x + beta_0, type = "male")
plot_data <- rbind(fe_data, ma_data)

text_data <- data.table(
  x = c(5, 12), y = c(6, 1),
  label = c(
    "wage == beta[0]+beta[2]*educ", "wage == beta[0]+sigma[f]+(beta[2]+gamma)*educ"
  )
)

g_comp_i <- ggplot(data = plot_data) +
  geom_line(aes(x = x, y = y, color = type)) +
  geom_text(
    data = text_data, aes(x, y, label = label),
    size = 6, parse = TRUE, family = "Times"
  ) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  ylab("hourly wage") +
  xlab("education") +
  scale_color_discrete(name = "") +
  theme(
    legend.position = "bottom"
  )

g_comp_i
```

---

# Categorical variable: more than two states


.content-box-green[**Issue**]

+ Consider a variable called $degree$ which has three status values:  college, master, and doctor.

+ Unlike a binary variable, there are three status values. 

+ How do we include a categorical variable like this in a model?

---

.content-box-green[**What do we do about this?**]

You can create three dummy variables likes below:

+ `college`: 1 if the highest degree is college, 0 otherwise
+ `master`: 1 if the highest degree is Master's, 0 otherwise
+ `doctor`: 1 if the highest degree is Ph.D., 0 otherwise

--

You then include two (the number of status values - 1) of the three dummy variables:

---

.content-box-red[**Model**]

$wage = \beta_0 + \sigma_m master +\sigma_d doctor + \beta_1 educ + u$

--

+ [college]: $E[wage|master=0, doctor = 0, educ] = \beta_0 + \beta_1 educ$

--

+ [master]: $E[wage|master=1, doctor = 0, educ] = \beta_0 + \sigma_m + \beta_1 educ$

--

+ [doctor]: $E[wage|master=0, doctor = 1, educ] = \beta_0 + \sigma_d + \beta_1 educ$

--

<br>

.content-box-red[**Interpretation**]

$\sigma_m$: the impact of having a MS degree .red[relative to] having a .red[college degree] 

$\sigma_d$: the impact of having a Ph.D. degree .red[relative to] having a .red[college degree]

--

<br>

.content-box-red[**Important**]

The omitted category (here, `college`) becomes the baseline.

---

# Structural differences across groups

.content-box-red[**Definition**]

Structural difference refers to the fundamental differences in the model of a phenomenon in the population:

---

.content-box-red[**Example**]

.blue[Male]: $cumgpa = \alpha_0 + \alpha_1 sat + \alpha_2 hsperc + \alpha_3 tothrs + u$

.blue[Female]: $cumgpa = \beta_0 + \beta_1 sat + \beta_2 hsperc + \beta_3 tothrs + u$

+ $cumgpa$: college grade points averages for male and female college athletes

+ $sat$: SAT score

+ $hsperc$: high school rank percentile

+ $tothrs$: total hours of college courses

--

<br>

.content-box-red[**In this example,**]

$cumgpa$ are determined in a fundamentally different manner between female and male students.

You do not want to run a single regression that fits a single model for both female and male students. 

---

.content-box-red[**What to do?**]

If you suspect that the underlying process of how the dependent variable is determined vary across groups, then you should test that hypothesis!

<br>

.content-box-red[**To do so,**]

You estimate the model that allows to estimate separate models across groups within a single regression analysis.     

$$cumgpa = \beta_0 + \sigma_0 female + \beta_1 sat + \sigma_1 (sat \times female)$$
          $$\;\; + \beta_2 hsperc + \sigma_2 (hsperc \times female)$$
          $$\qquad + \beta_3 tothrs + \sigma_3 (tothrs \times female) + u$$

---

.content-box-red[**The flexible model**]

$$cumgpa = \beta_0 + \sigma_0 female + \beta_1 sat + \sigma_1 (sat \times female)$$
          $$\;\; + \beta_2 hsperc + \sigma_2 (hsperc \times female)$$
          $$\qquad + \beta_3 tothrs + \sigma_3 (tothrs \times female) + u$$


<br>

.content-box-green[**Male**]

$E[cumgpa] = \beta_0 + \beta_1 sat + \beta_2 hsperc + \beta_3 tothrs$

<br>

.content-box-green[**Feale**]

$E[cumgpa] = (\beta_0 +\sigma_0) + (\beta_1+\sigma_1) sat + (\beta_2+\sigma_2) hsperc + (\beta_3+\sigma_3) tothrs$

<br>

.content-box-red[**Interpretation**]

+ $\beta$s are commonly shared by female and male students 
+ $\sigma$s capture the differences between female and male students 

---

.content-box-red[**Null Hypothesis (verbal)**]

The model of GPA for male and female students are not structurally different.

<br>

.content-box-red[**Null Hypothesis**]

$H_0: \;\; \sigma_0=0,\;\; \sigma_1=0, \;\; \sigma_2=0, \;\; \mbox{and} \;\; \sigma_3=0$

<br>

.content-box-green[**Question**]

What test do we do? t-test or F-test?

---
class: middle

.content-box-red[**R code**]

Run the unrestricted model with all the interaction terms:

```{r }
data("gpa3", package = "wooldridge")

gpa <-
  gpa3 %>%
  dplyr::filter(!is.na(ctothrs)) %>%
  #--- create interaction terms ---#
  dplyr::mutate(
    female_sat := female * sat,
    female_hsperc := female * hsperc,
    female_tothrs := female * tothrs
  )

#--- regression with female dummy ---#
reg_full <-
  fixest::feols(
    cumgpa ~
      female + sat + female_sat + hsperc + female_hsperc +
      tothrs + female_tothrs,
    data = gpa
  )
```

---
class: middle

.left5[

.content-box-red[**What do you see?**]

+ None of the variables that involve $female$ are statistically significant at the 5% level individually. 

+ Does this mean that $male$ and $female$ students have the same regression function?

+ No, we are testing the joint significance of the coefficients. We need to do an $F$-test!

]

.right5[
```{r echo = F}
modelsummary::msummary(
  reg_full,
  gof_omit = "Obs|R|Log|IC|Std",
  stars = TRUE
)
```
]

---

```{r }
car::linearHypothesis(
  reg_full,
  c(
    "female = 0",
    "female_hsperc = 0",
    "female_sat = 0",
    "female_tothrs = 0"
  )
)
```

---
class: inverse, center, middle
name: use-i

# R coding tips: categorical variables and interaction terms

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---
class: middle

# R coding tips: categorical variables and interaction terms

```{r }
#* load the package to access the data we want
library(wooldridge)

#* get big9salary
data("big9salary")

#* creat a variable that indicates university
#* this is how the data would like most of the time (instead of having bunch of dummy variables)
big9salary_c <-
  as_tibble(big9salary) %>%
  dplyr::mutate(
    university =
      case_when(
        osu == 1 ~ "Ohio State U",
        iowa == 1 ~ "U of Iowa",
        indiana == 1 ~ "Indiana U",
        purdue == 1 ~ "Purdue U",
        msu == 1 ~ "Michigan State U",
        mich == 1 ~ "Michigan U",
        wisc == 1 ~ "U of Wisconsin",
        illinois == 1 ~ "U of Illinois"
      )
  ) %>%
  dplyr::relocate(id, year, salary, pubindx, university)
```

---
class: middle

Take a look at the data,

```{r }
head(big9salary_c)

tail(big9salary_c)
```

---
class: middle

You can use the `i()` function inside `fixest::feols()` like below:

```{r eval = F}
fixest::feols(salary ~ pubindx + female + `i(university, ref = "Indiana U")`, data = big9salary_c) %>%
  broom::tidy()
```

```{r echo = F, highlight.output = c(7)}
fixest::feols(salary ~ pubindx + female + i(university, ref = "Indiana U"), data = big9salary_c) %>%
  broom::tidy()
```

`ref = "Indiana U"` sets the base category to `"Indiana U"`. 

So, for example, the highlighted line means that faculty memebers at Michigan State U make $9,118$ USD less annually than those at Indiana U.

<br>

.content-box-green[**Key**]

You do not have to make bunch of dummy variables like the original dataset. Just use `i(catergory_variable)`.

---
class: middle

# Interactions terms 

You can use `i()` for creating interactions of a categorical variable and a continuous variable.

Suppose you are interested in understanding the impact of `pubindx` (continuous) by `university` (categorical), then

```{r eval = F}
fixest::feols(salary ~ female + pubindx + i(university, ref = "Indiana U") + `i(university, totpge, ref = "Indiana U")`, data = big9salary_c) %>%
  broom::tidy()
```

```{r echo = F, highlight.output = c(14)}
fixest::feols(salary ~ female + pubindx + i(university, ref = "Indiana U") + i(university, pubindx, ref = "Indiana U"), data = big9salary_c) %>%
  broom::tidy()
```

So, the marginal impact of `pubindex` is $436$ greater for those at Michigan State U than those at Indiana U.

---

class: inverse, center, middle
name: misc

# Other miscellaneous topic

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---

# Goodness of fit: $R^2$

.content-box-red[**Important**]

Small value of $R^2$ does not mean the end of the world (In fact, we could not care less about it.)

---

.content-box-green[**Example**]

$$ecolabs = \beta_0 + \beta_1 regprc + \beta_2 ecoprc$$

+ $ecolabs$: the (hypothetical) pounds of ecologically friendly (ecolabled) apples a family would demand
+ $regprc$: prices of regular apples 
+ $ecoprc$: prices of the hypothetical ecolabled apples

<br>

.content-box-red[**Key**]
+ The data was obtained via survey and $ecoprc$ was set randomly (So, we know $E[u|x] = 0$) by the researcher.
+ The (only) objective of the study is to understand the impact of the price of ecolabled apple on the demand for ecolabled apples.


---

```{r echo = F, results = "asis"}
apple <- read.dta13("APPLE.dta")
reg_apple <- fixest::feols(ecolbs ~ regprc + ecoprc, data = apple)
stargazer(reg_apple, type = "html", no.space = TRUE, omit.stat = c("adj.rsq", "ser", "f"), table.layout = "-ld#-t-s-")
```
 
---

Suppose you are challenged by somebody who claim that your regression is not .blue[good] because the $R^2$ is tiny. How would your respond to his/her attack?

---

class: inverse, center, middle
name: review

# Scaling

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---


.content-box-green[**Questions**]

What happens if you scale up/down variables used in regression? 

+ coefficients
+ standard errors
+ t-statistics
+ $R^2$

---

```{r scale-no-scale}
#--- regression with original scale ---#
reg_no_scale <- fixest::feols(wage ~ female + educ, data = wage)

#--- regression with scaled educ ---#
reg_scale <- fixest::feols(wage ~ female + I(educ * 12), data = wage)
```
  
---

.left5[
```{r dependson = "scale-no-scale", eval = F}
modelsummary::msummary(
  list(reg_no_scale, reg_scale),
  stars = TRUE,
  gof_omit = "IC|Log|Adj|F|Pseudo|Within"
)
```

<br>

.content-box-green[**So,**]

+ coefficient: 1/12
+ standard error: 1/12
+ t-stat: the same
+ $R^2$: the same

]

.right5[
```{r dependson = "scale-no-scale", echo = F}
modelsummary::msummary(
  list(reg_no_scale, reg_scale),
  stars = TRUE,
  gof_omit = "IC|Log|Adj|F|Pseudo|Within"
)
```
]

---

.content-box-red[**Interpretation**]

+ Regression .blue[without] scaling 

hourly wage increases by $0.506$ if education increases by a .blue[year]

+ Regression .blue[with] scaling (e.g., 48 means 4 years)

hourly wage increases by $0.0422$ if education increases by a .blue[month]

--

<br>

.content-box-green[**Note**]

According to the scaled model, hourly wage increases by $0.0422 * 12$ if education increases by a year (12 months).

That is, the estimated marginal impact of education on wage from the scaled model is the same as that from the non-scaled model.

---

.content-box-red[**Summary**]

When an independent variable is scaled, 

+ its coefficient estimate and standard error are going to be scaled up/back to the exact degree the variable is scaled up/back
+ t-statistics stays the same (as it should be)
+ $R^2$ stays the same (the model does not improve by simply scaling independent variables)

