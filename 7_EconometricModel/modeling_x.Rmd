---
title: "Econometric Modeling"
author: "AECN 396/896-002"
output:
  xaringan::moon_reader:
    # css: [default, metropolis, metropolis-fonts] 
    css: xaringan-themer.css 
    # lib_dir: libs
    self_contained: true
    nature:
      ratio: 4:3
      highlightStyle: github
      highlightSpans: true
      highlightLines: true
      countIncrementalSlides: false
---
class: middle
layout: true

---

```{r, child = './../setup.Rmd'}
```

```{r echo = F, eval = F}
setwd("/Users/tmieno2/Dropbox/TeachingUNL/AppliedEconometrics_MS/LectureNotes/EconometricModel/")
```

```{r packages, include = F, cache = F}
#--- load packages ---#
suppressMessages(library(broom))
suppressMessages(library(fixest))
suppressMessages(library(patchwork))
suppressMessages(library(readstata13))
suppressMessages(library(modelsummary))
suppressMessages(library(stargazer))
suppressMessages(library(lmtest))
```

# Before we start 

## Learning objectives

1. Enhance the understanding of the interpretation of various models
1. Post-estimation simulation

## Table of contents

1. [](#review)
2. [](#linear)
3. [](#conf)

---

class: inverse, center, middle
name: review

# More on functional forms

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---

# Various econometric models  

.content-box-red[**log-linear**]

$log(y_i)= \beta_0+\beta_1 x_i + u_i$ 

.content-box-red[**linear-log**]

$y_i= \beta_0+\beta_1 log(x_i) + u_i$ 

.content-box-red[**log-log**]

$log(y_i)= \beta_0+\beta_1 log(x_i) + u_i$ 

--

.content-box-red[**quadratic**]

$y_i= \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + u_i$ 

---

# Quadratic 


.content-box-red[**Model**]

$y_i= \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + u_i$ 

--

.content-box-red[**Calculus**]

Differentiating the both sides wrt $x_i$,

$\frac{\partial y_i}{\partial x_i} = \beta_1 + 2*\beta_2 x_i\Rightarrow  \Delta y_i = (\beta_1 + 2*\beta_2 x_i)\Delta x_i$


.content-box-red[**Interpretation**]

When $x$ increases by 1 unit $(\Delta x_i=1)$, $y$ increases by $\beta_1 + 2*\beta_2 x_i$

---

# Visualizarion 

Quadratic functional form is quite flexible.

```{r echo = F}
N <- 1000
x <- seq(0,2,length=N)
y_1 <- x+x^2
y_2 <- 3*x-2*x^2
y_1_data <- data.table(
  y = y_1,
  x=x
  )
y_2_data <- data.table(
  y = y_2,
  x=x
  )

g_quad_1 <- ggplot(data = y_1_data) +
  geom_line(aes(y = y, x = x), color = "red")

g_quad_2 <- ggplot(data = y_2_data) +
  geom_line(aes(y = y, x = x), color = "red")

```

.left5[

$y = x + x^2$ $(\beta_1 = 1, \beta_2 = 1)$

```{r echo = F}
g_quad_1
```
]

.right5[

$y = 3x-2x^2$ $(\beta_1 = 3, \beta_2 = -2)$

```{r echo = F}
g_quad_2
```
]

---

# Example

.content-box-red[**Education impacts of income**]

The marginal impact of education (the impact of a small change in education on income) may differ what level of education you have had:

+ How much does it help to have two more years of education when you have had education until elementary school?

+ How much does it help to have two more years of education when you have have graduated a college?

+ How much does it help to spend two more years as a Ph.D student if you have already spent six years in a Ph.D program

--

.content-box-green[**Observation**]

The marginal impact of education does not seem to be linear.

---

# Implementation in R

When you include a variable that is a transformation of an existing variable, use `I()` function in which you write the mathematical expression of the desired transformation.

```{r }
#--- prepare a dataset ---#
wage <- readRDS('wage1.rds') 

#--- run a regression ---#
quad_reg <- feols(wage ~ female + educ + I(educ^2), data = wage)

#--- look at the results ---#
tidy(quad_reg)
```

---

.content-box-red[**Estimated Model**]

$wage = 5.60 - 2.12\times female -0.416\times educ + 0.039\times educ^2$

```{r echo = F, out.width = "60%"}
N <- 1000
x <- seq(min(wage$educ), max(wage$educ), length = N)
data <- data.table(educ = x, female = 1) %>% 
  mutate(y = predict(quad_reg, newdata = .))

g_quad_ex <- ggplot() +
  geom_point(data = wage, aes(y = wage, x = educ), size = 0.4) +
  geom_line(data = data, aes(y = y, x = x)) 

g_quad_ex 
```

---

.content-box-red[**Estimated Model**]

$wage = 5.60 - 2.12\times female -0.416\times educ + 0.039\times educ^2$

<br>

--

.content-box-green[**Problem**]

What is the marginal impact of $educ$?

$\frac{\partial wage}{\partial educ} = ?$

--

.content-box-green[**Answer**]

$\frac{\partial wage}{\partial educ} = -0.416+0.039\times 2\times educ$

--

+ When $educ = 4$, additional year of education is going to increase hourly wage by `r -0.416+0.078*4` on average 

--

+ When $educ = 10$, additional year of education is going to increase hourly wage by `r -0.416+0.078*10` on average

---

# Statistical significance of the marginal impact 

The marginal impact of $educ$ is:

$\frac{\partial wage}{\partial educ} = -0.416+0.039\times 2\times educ$

+ $educ$: $-0.416$ $(t$-stat $= -1.80)$
+ $educ^2$: $0.039$ $(t$-stat $= 4.10)$

--

<br>

.content-box-green[**Question**]

So, is the marginal impact of $educ$ statistically significantly different from $0$?

---

# In the linear case 

```{r }
linear_reg <- feols(wage ~ female + educ, data = wage)
tidy(linear_reg) 
```

--

.content-box-red[**Estimated model**]

$wage = 0.62+0.51 \times educ$

---
class: middle

.content-box-red[**Estimated model**]

$wage = 0.62+0.51 \times educ$

--

.content-box-green[**Question**]

+ What is the marginal impact of $educ$?

--

0.51

--

+ Does the marginal impact of education vary depending on the level of education? 

--

No, the model we estimated assumed that the marginal impact of education is constant.

--

.content-box-red[**Testing**]

You can just test if $\hat{\beta}_{educ}$ (the marginal impact of education) is statistically significantly different from $0$, which is just a t-test.

---

# Going back to the quadratic case 

With the quadratic specification  

+ The marginal impact of education varies depending on your education level

--

+ There is no single test that tells you whether the marginal impact of education is statistically significant universally

--

+ Indeed, you need different tests for different values education levels

---

# Example 1


.content-box-red[**Marginal impact of education**]

$\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times educ$

--

.content-box-red[**Hypothesis testing**]

Does additional year of education has a statistically significant impact (positive or negative) if your current education level is 4?    
+ $H_0$: $\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 4 =0$

+ $H_1$: $\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 4 \ne 0$

--
    
.content-box-green[**Question**]

Is this

+ test of a single coefficient? (t-test)
+ test of a single equation with multiple coefficients? (t-test)
+ test of multiples equations with multiple coefficients? (F-test)

---

.content-box-red[**t-statistic**]

$t = \frac{\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 4}{se(\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 4)} = \frac{\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 8}{se(\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 8)}$

--

.content-box-red[**R implementation**]

Remember, a trick to do this test using R is take advantage of the fact that $F_{1, n-k-1} \sim t_{n-k-1}$.

```{r }
linearHypothesis(quad_reg, "educ + 8*I(educ^2)=0")  
```

--

Since the p-value is 0.529, we do not reject the null.


# Example 2

.content-box-red[**Marginal impact of education**]

$\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times educ$

--

.content-box-red[**Hypothesis testing**]

Does additional year of education has a statistically significant impact (positive or negative) if your current education level is 4?    
+ $H_0$: $\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 10 =0$

+ $H_1$: $\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 10 \ne 0$

--
    
.content-box-green[**Question**]

Is this

+ test of a single coefficient? (t-test)
+ test of a single equation with multiple coefficients? (t-test)
+ test of multiples equations with multiple coefficients? (F-test)

---

.content-box-red[**t-statistic**]

$t = \frac{\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 10}{se(\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 2 \times 10)} = \frac{\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 20}{se(\hat{\beta}_{educ} + \hat{\beta}_{educ^2} \times 20)}$

--

.content-box-red[**R implementation**]

```{r }
linearHypothesis(quad_reg, "educ + 20*I(educ^2)=0")  
```

--

Since the much lower than is 0.01, we can reject the null at the 1% level.

---

class: inverse, center, middle
name: review

# Interaction terms

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---
class: middle

.content-box-red[**An interaction term**]

A variable that is a multiplication of two variables 

--

.content-box-red[**Example**]

$educ\times exper$

---

.content-box-red[**A model with an interaction term**]

$wage = \beta_0 + \beta_1 exper + \beta_2 educ \times exper + u$ 

--

.content-box-red[**Marginal impact of experience**]

$\frac{\partial wage}{\partial exper} = \beta_1+\beta_2\times educ$ 

--

.content-box-red[**Implications**]
  
The marginal impact of experience depends on education

+ $\beta_1$: the marginal impact of experience when $educ=?$

+ if $\beta_2>0$: additional year of experience is worth more when you have more years of education

---

# Regression with interaction terms 

Just like the quadratic case with $educ^2$, you can use `I()`.

```{r }
reg_int <- feols(wage ~ female + exper + I(exper*educ), data = wage) 
```

--

```{r echo = F}
modelsummary(reg_int, gof_omit = "R|IC|F|Log|Obs", stars = T)
```
  
---
class: middle

.content-box-red[**Estimated Model**]

$wage = 6.121 - 2.418 \times female - 0.188 \times exper + 0.020 \times educ \times exper$ 

<br>

.content-box-red[**Marginal impact of experience**]

$\frac{\partial wage}{\partial exper} = - 0.188 + 0.020 \times educ$   
---
class: middle

<br>
<br>

```{r echo = F}
N <- 1000
educ_temp <- seq(min(wage$educ),max(wage$educ),length=N) 
min_exper <- reg_int$coefficients['exper'] + reg_int$coefficients['I(exper * educ)']*educ_temp
data_exper <- data.table(
  x = educ_temp,
  y = min_exper
  )

g_marginal <- ggplot(data=data_exper) +
  geom_line(aes(x=x,y=y)) +
  ylab('marginal impact of experience') +
  xlab('education') +
  geom_hline(yintercept=0)

g_hist <- ggplot(data=wage) +
  geom_histogram(aes(educ)) +
  xlab('education')   
```

.left5[

Marginal impact of $exper$:

```{r echo = F}
g_marginal 
```
]

.right5[
Histogram of education:
```{r echo = F}
g_hist 
```
]

---
class: middle

.content-box-red[**Testing of the marginal impact**]


+ Just like the case of the quadratic specification of education, marginal impact of experience is not constant

+ We can test if the marginal impact of experience is statistically significant for a given level of education

  * When $educ=10$, $\frac{\partial wage}{\partial exper} = - 0.188 + 0.020 \times 10=0.012$
  * When $educ=15$, $\frac{\partial wage}{\partial exper} = - 0.188 + 0.020 \times 15=0.112$

---
class: middle

.content-box-red[**Question**]

Does additional year of experience has a statistically significant impact (positive or negative) if your current education level is 10?

.content-box-red[**Hypothesis**]
      
+ $H_0$: $\hat{\beta}_{exper} + \hat{\beta}_{exper\_educ} \times 10=0$

+ $H_1$: $\hat{\beta}_{exper} + \hat{\beta}_{exper\_educ} \times 10=0$

---
class: middle

.content-box-red[**R implementation**]

```{r }
linearHypothesis(reg_int,'exper+10*I(exper * educ)=0') 
```

---

class: inverse, center, middle
name: review

# Including qualitative information

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---

# Qualitative information  

.content-box-green[**Issue**]

How do we include qualitative information as an independent variable?

--

.content-box-green[**Examples**]

+ male or female (binary)

+ married or single (binary)

+ high-school, college, masters, or Ph.D (more than two states)

---

# Binary variables 


.content-box-red[**Dummy variable**]

+ Relevant information in binary variables can be captured by a .red[zero-one] variable that takes the value of $1$ for one state and $0$ for the other state

+ We use "dummy variable" to refer to a binary (zero-one) variable

.content-box-red[**Example**]

```{r }
wage <- readRDS('wage1.rds') 

dplyr::select(wage, wage, educ, exper, female, married) %>% 
    head() 
```

---
class: middle

.content-box-red[**Model with dummy a variable**]

$wage = \beta_0 +\sigma_f female +\beta_2 educ + u$


.content-box-red[**Interpretation**]

+ `female`: $E[wage|female=1,educ] = \beta_0 + \sigma_f +\beta_2 educ$

+ `male`: $E[wage|female=0,educ] = \beta_0 + \beta_2 educ$

--

This means that 

$\sigma_f = E[wage|female=1,educ]-E[wage|female=0,educ]$

---
class: middle

$\sigma_f = E[wage|female=1,educ]-E[wage|female=0,educ]$

Verbally,

+ $\sigma_f$ is the difference in the expected wage conditional on education between female and male

+ $\sigma_f$ measures how much more (less) female workers make compared to male workers (.blue[baseline]) if they were to have the same education level 

---
class: middle


.content-box-red[**Regression with a dummy variable**]

```{r }
reg_df <- feols(wage ~ female + educ, data = wage) 

reg_df
```


.content-box-red[**Interpretation**]

Female workers make `r reg_df$coefficients["female"]` ($/hour) less than male workers on average even though they have the same education level. 

---


.content-box-red[**Visualization of the estimated model**]

```{r echo = F, fig.dim = c(6, 4)}
beta_0 <- reg_df$coefficients[1] # intercept
sigma_0 <- reg_df$coefficients['female'] # coef on female
beta_1 <- reg_df$coefficients['educ'] # coef on educ
x <- seq(0,20,length=1000)
fe_data <- data.table(x=x,y=beta_1*x+beta_0+sigma_0,type='female')
ma_data <- data.table(x=x,y=beta_1*x+beta_0,type='male')
plot_data <- rbind(fe_data,ma_data)

text_data <- data.table(
  x=c(5,12),y=c(6,1),
  label=c(
    'wage == beta[0]+beta[2]*educ','wage == beta[0]+sigma[f]+beta[2]*educ')
  )

g_comp <- ggplot(data=plot_data) + 
  geom_line(aes(x=x,y=y,color=type)) +
  geom_text(data=text_data,aes(x,y,label=label),
    size=6,parse=TRUE,family='Times')+
  geom_vline(xintercept=0) +
  geom_hline(yintercept=0) +
  ylab('Hourly Wage ($/hour)') +
  xlab('Education (years)') +
  scale_color_discrete(name='') +
  theme(
    legend.position='bottom',
    text = element_text(size = 16)
    )
g_comp 
```

---
class: middle

.content-box-red[**Model with dummy a variable**]

$wage = \beta_0 +\sigma_m male +\beta_2 educ + u$

.content-box-red[**Interpretation**]

+ `male`: $E[wage|male = 1,educ] = \beta_0 + \sigma_m +\beta_2 educ$

+ `female`: $E[wage|male = 0,educ] = \beta_0 + \beta_2 educ$

--

This means that 

$\sigma_m = E[wage|male=1,educ]-E[wage|male=0,educ]$

---
class: middle

$\sigma_m = E[wage|male=1,educ]-E[wage|male=0,educ]$

Verbally,

+ $\sigma_m$ is the difference in the expected wage conditional on education between female and male

+ $\sigma_m$ measures how much more (less) male workers make compared to female workers (.blue[baseline]) if they were to have the same education level 

.red[Important]: whichever status that is given the value of $0$ becomes the baseline

---
class: middle


.content-box-red[**Regression with a dummy variable**]

```{r }
wage <- mutate(wage, male = 1- female)

reg_df <- feols(wage ~ male + educ, data = wage) 

reg_df
```


.content-box-red[**Interpretation**]

Female workers make `r reg_df$coefficients["male"]` ($/hour) more than female workers on average even though they have the same education level. 

---

class: middle

.content-box-red[**Question**]

Why do you think will happen if  we include both male and female dummy variables?   

--

.content-box-red[**Answer**]

+ They contain redundant information

+ Indeed, including both of them along with the intercept would cause .blue[perfect collinearity problem]

+ So, you .blue[need to] drop either one of them

--

.content-box-red[**Perfect Collinearity**]

intercept = male + female

---
class: middle

Here is what happens if you include both:

```{r highlight.output = c(8)}
reg_dmf <- feols(wage ~ male + female + educ, data = wage) 

reg_dmf 
```

---
class: middle

# Interactions with a dummy variable 


.content-box-green[**Issue**]

+ In the previous example, the impact of education on wage was modeled to be exactly the same

+ Can we build a more flexible model that allows us to estimate the differential impacts of education on wage between male and female?

---
class: middle

.content-box-red[**A more flexible model**]

$wage = \beta_0 + \sigma_f female +\beta_2 educ + \gamma female\times educ + u$ 

+ [female]: $E[wage|female=1,educ] = \beta_0 + \sigma_f +(\beta_2+\gamma) educ$
+ [male]: $E[wage|female=0,educ] = \beta_0 + \beta_2 educ$


.content-box-red[**Interpretation**]

For female, education is more effective by $\gamma$ than it is for male.

---


.content-box-red[**Example using R**]

```{r eval = F}
reg_di <- lm(wage ~ female + educ + `I(female * educ)`, data = wage) 
reg_di 
```

```{r echo = F}
reg_di <- lm(wage ~ female + educ + I(female * educ), data = wage) 
reg_di 
```

--

.content-box-red[**Interpretation**]

The marginal benefit of education is 0.086 ($/hour) less for females workers than for male workers on average.

---

```{r echo = F, fig.dim = c(6, 4.5), out.width = "80%"}
beta_0 <- reg_di$coefficients[1] # intercept
sigma_0 <- reg_di$coefficients['female'] # coef on female
beta_1 <- reg_di$coefficients['educ'] # coef on educ
gamma <- reg_di$coefficients['I(female * educ)']
x <- seq(0,20,length=1000)
fe_data <- data.table(x=x,y=(beta_1+gamma)*x+beta_0+sigma_0,type='female')
ma_data <- data.table(x=x,y=beta_1*x+beta_0,type='male')
plot_data <- rbind(fe_data,ma_data)

text_data <- data.table(
  x=c(5,12),y=c(6,1),
  label=c(
    'wage == beta[0]+beta[2]*educ','wage == beta[0]+sigma[f]+(beta[2]+gamma)*educ')
  )

g_comp_i <- ggplot(data=plot_data) + 
  geom_line(aes(x=x,y=y,color=type)) +
  geom_text(data=text_data,aes(x,y,label=label),
    size=6,parse=TRUE,family='Times')+
  geom_vline(xintercept=0) +
  geom_hline(yintercept=0) +
  ylab('hourly wage') +
  xlab('education') +
  scale_color_discrete(name='') +
  theme(
    legend.position='bottom'
    )

g_comp_i 
```

---

# Categorical variable: more than two states


.content-box-green[**Issue**]

+ Consider a variable called $degree$ which has three status values:  college, master, and doctor.

+ Unlike a binary variable, there are three status values. 

+ How do we include a categorical variable like this in a model?

---

.content-box-green[**What do we do about this?**]

You can create three dummy variables likes below:

+ `college`: 1 if the highest degree is college, 0 otherwise
+ `master`: 1 if the highest degree is Master's, 0 otherwise
+ `doctor`: 1 if the highest degree is Ph.D., 0 otherwise

--

You then include two (the number of status values - 1) of the three dummy variables:

---

.content-box-red[**Model**]

$wage = \beta_0 + \sigma_m master +\sigma_d doctor + \beta_1 educ + u$

--

+ [college]: $E[wage|master=0, doctor = 0, educ] = \beta_0 + \beta_1 educ$

--

+ [master]: $E[wage|master=1, doctor = 0, educ] = \beta_0 + \sigma_m + \beta_1 educ$

--

+ [doctor]: $E[wage|master=0, doctor = 1, educ] = \beta_0 + \sigma_d + \beta_1 educ$

--

.content-box-red[**Interpretation**]

$\sigma_m$: the impact of having a MS degree .red[relative to] having a .red[college degree] 

$\sigma_d$: the impact of having a Ph.D. degree .red[relative to] having a .red[college degree]

--

.content-box-red[**Important**]

The omitted category (here, `college`) becomes the baseline.

---

# Structural differences across groups

.content-box-red[**Definition**]

Structural difference refers to the fundamental differences in the model of a phenomenon in the population:

---

.content-box-red[**Example**]

.blue[Male]: $cumgpa = \alpha_0 + \alpha_1 sat + \alpha_2 hsperc + \alpha_3 tothrs + u$

.blue[Female]: $cumgpa = \beta_0 + \beta_1 sat + \beta_2 hsperc + \beta_3 tothrs + u$

+ $cumgpa$: college grade points averages for male and female college athletes

+ $sat$: SAT score

+ $hsperc$: high school rank percentile

+ $tothrs$: total hours of college courses

--

.content-box-red[**In this example,**]

$cumgpa$ are determined in a fundamentally different manner between female and male students.

You do not want to run a single regression that fits a single model for both female and male students. 

---

.content-box-red[**What to do?**]

If you suspect that the underlying process of how the dependent variable is determined vary across groups, then you should test that hypothesis!

.content-box-red[**To do so,**]

You estimate the model that allows to estimate separate models across groups within a single regression analysis.     

$$cumgpa = \beta_0 + \sigma_0 female + \beta_1 sat + \sigma_1 (sat \times female)$$
          $$\;\; + \beta_2 hsperc + \sigma_2 (hsperc \times female)$$
          $$\qquad + \beta_3 tothrs + \sigma_3 (tothrs \times female) + u$$

---


.content-box-red[**The flexible model**]

$$cumgpa = \beta_0 + \sigma_0 female + \beta_1 sat + \sigma_1 (sat \times female)$$
          $$\;\; + \beta_2 hsperc + \sigma_2 (hsperc \times female)$$
          $$\qquad + \beta_3 tothrs + \sigma_3 (tothrs \times female) + u$$


.content-box-green[**Male**]

$E[cumgpa] = \beta_0 + \beta_1 sat + \beta_2 hsperc + \beta_3 tothrs$

.content-box-green[**Feale**]

$E[cumgpa] = (\beta_0 +\sigma_0) + (\beta_1+\sigma_1) sat + (\beta_2+\sigma_2) hsperc + (\beta_3+\sigma_3) tothrs$


.content-box-red[**Interpretation**]

+ $\beta$s are commonly shared by female and male students 
+ $\sigma$s capture the differences between female and male students 

---

.content-box-red[**Hypothesis (verbal)**]
The model of GPA for male and female students are not structurally different.

.content-box-red[**Null Hypothesis**]

$H_0: \;\; \sigma_0=0,\;\; \sigma_1=0, \;\; \sigma_2=0, \;\; \mbox{and} \;\; \sigma_3=0$


.content-box-green[**Question**]

What test do we do? t-test or F-test?

---


.content-box-red[**R code**]

Run the unrestricted model with all the interaction terms:

```{r }
gpa <- read.dta13("GPA3.dta") %>% 
    filter(!is.na(ctothrs)) %>%
    #--- create interaction terms ---#
    mutate(
      female_sat:=female*sat,
      female_hsperc:=female*hsperc,
      female_tothrs:=female*tothrs
      )

#--- regression with female dummy ---#
reg_full <- feols(cumgpa~female
  +sat+female_sat
  +hsperc+female_hsperc
  +tothrs+female_tothrs,
  data=gpa
)   
```

---

.left5[

.content-box-red[**What do you see?**]

+ None of the variables that involve $female$ are statistically significant at the 5% level individually. 

+ Does this mean that $male$ and $female$ students have the same regression function?

+ No, we are testing the joint significance of the coefficients. We need to do an $F$-test!

]

.right5[
```{r echo = F}
modelsummary(reg_full, gof_omit = "Obs|R|Log|IC|Std", stars = TRUE)
```
]

---

```{r }
linearHypothesis(reg_full, 
  c(
    "female = 0", 
    "female_hsperc = 0", 
    "female_sat = 0", 
    "female_tothrs = 0"
  )
)  
```

---

class: inverse, center, middle
name: review

# More on $R^2$ 

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---

# Goodness of fit: $R^2$ 

.content-box-red[**Important**]

Small value of $R^2$ does not mean the end of the world (In fact, we could not care less about it.)

---

.content-box-green[**Example**]

$$ecolabs = \beta_0 + \beta_1 regprc + \beta_2 ecoprc$$

+ $ecolabs$: the (hypothetical) pounds of ecologically friendly (ecolabled) apples a family would demand
+ $regprc$: prices of regular apples 
+ $ecoprc$: prices of the hypothetical ecolabled apples

.content-box-red[**Key**]
+ The data was obtained via survey and $ecoprc$ was set randomly (So, we know $E[u|x] = 0$).
+ The objective of the study is to understand the impact of the price of ecolabled apple on the demand for ecolabled apples.


---

```{r echo = F, results = "asis"}
apple <- read.dta13('APPLE.dta') 
reg_apple <- lm(ecolbs~regprc+ecoprc,data=apple)
stargazer(reg_apple,type='html',no.space = TRUE,omit.stat=c('adj.rsq','ser','f'),table.layout='-ld#-t-s-') 
```
 
---

Suppose you are challenged by somebody who claim that your regression is not .blue[good] because the $R^2$ is tiny. How would your respond to his/her attack?

---

class: inverse, center, middle
name: review

# Scaling 

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---


.content-box-green[**Questions**]

What happens if you scale up/down variables used in regression? 

+ coefficients
+ standard errors
+ t-statistics
+ $R^2$

---

```{r }
#--- regression with female dummy ---#
reg_no_scale <- lm(wage ~ female + educ, data = wage) 
reg_scale <- lm(wage ~ female + I(educ * 12), data = wage)  
```
  
---

```{r }
tidy(reg_no_scale) 
```

```{r }
tidy(reg_scale) 
```

--


.content-box-green[**So,**]

+ coefficient: 1/12
+ standard error: 1/12
+ t-stat: the same

---

.content-box-red[**Interpretation**]

+ Regression .blue[without] scaling 

hourly wage increases by $0.506$ if education increases by a .blue[year]

+ Regression .blue[with] scaling (e.g., 48 means 4 years)

hourly wage increases by $0.0422$ if education increases by a .blue[month]

--

.content-box-green[**Note**]

According to the scaled model, hourly wage increases by $0.0422 * 12$ if education increases by a year (12 months).

That is, the estimated marginal impact of education on wage from the scaled model is the same as that from the unscaled model.

---


.content-box-red[**Summary**]

When an independent variable is scaled, 

+ its coefficient estimate and standard error are going to be scaled up/back to the exact degree the variable is scaled up/back
+ t-statistics stays the same (as it should be)
+ $R^2$ stays the same (the model does not improve by simply scaling independent variables)

