\documentclass[fleqn]{beamer}

% \usefonttheme[onlylarge]{structuresmallcapsserif}
\usefonttheme[onlysmall]{structurebold}
\usepackage[normalem]{ulem} % use normalem to protect \emph
\newcommand\hl{\bgroup\markoverwith
  {\textcolor{yellow}{\rule[-.5ex]{2pt}{2.5ex}}}\ULon}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{lscape}
\usepackage[authoryear]{natbib}
\usepackage{array}
\usepackage{listings}
\usepackage{wasysym}
\usepackage{url}
\usepackage{multirow}
\usepackage{color}
\usepackage{qtree}
\usepackage[absolute,overlay]{textpos}
\usepackage{inconsolata}
\usepackage{framed}
\definecolor{shadecolor}{rgb}{1,0.8,0.3}
\usepackage[justification=centering]{caption}
\captionsetup{compatibility=false}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue}
\usepackage[space]{grffile}
\usefonttheme[onlymath]{serif}
% \usepackage{tikz}
% \usetikzlibrary{calc,matrix}
\usepackage{sgame}
\usepackage{tikz}
\usetikzlibrary{trees}

\mode<presentation>
%\usetheme{Frankfurt}
\usetheme{boxes}
\usecolortheme{beaver}
%\usetheme{Singapore}
%\usetheme{Hannover}

\newcommand*{\captionsource}[2]{%
  \caption[{#1}]{%
    #1%
    \\\hspace{\linewidth}%
    {\small\textbf{Source:} #2}%
  }%
}

\beamertemplatenavigationsymbolsempty
\setbeamertemplate{blocks}[rounded]
\newenvironment<>{block_code}[1]{%
  \setbeamercolor{block title}{fg=white,bg=black}%
  \begin{block}#2{#1}}{\end{block}}

%--------------------------
% Reduce the spacing between R code and output
%--------------------------
\usepackage{etoolbox}
\makeatletter
\preto{\@verbatim}{\topsep=0pt \partopsep=0pt }
\makeatother

\renewenvironment{knitrout}{\setlength{\topsep}{0mm}}{}

%===================================
% Coloring change
%===================================
\definecolor{darkred}{rgb}{0.8,0,0}
\setbeamercolor{block title}{fg=darkred,bg=structure.fg!20!bg!50!bg}
\setbeamercolor{block body}{use=block title,bg=block title.bg}

\title{Panel Data Methods}
\author{Taro Mieno}
\date{AECN 896-003: Applied Econometrics}
\everymath{\displaystyle}

\begin{document}
\begin{frame}
\titlepage
\end{frame}

<<prep, echo=FALSE, message=F, warning=F>>=
source('~/Box/R_libraries_load/library.R')
library(lfe)
library(stargazer)
library(readstata13)
source('~/Box/MySoftware/MyR/ggplot2/ggplot2_default_teaching.R')
source('~/Box/Teaching/R_functions/functions.R')
opts_chunk$set(
  comment = NA,
  message = FALSE,
  warnings = FALSE,
  tidy=FALSE,
  size='footnotesize',
  #--- figure related ---#
  fig.align='center',
  out.height='2.5in',
  out.width='2.5in',
  dev='pdf'
  )

setwd('/Users/tmieno2/Box/Teaching/UNL/EconometricsMaster/aecn892_2019/lectures/PanelData')
@

%===================================
% Functional Form
%===================================
\begin{frame}[c]
  \title{Panel Data}
  \author{}
  \date{}
  \maketitle
\end{frame}

\begin{frame}[c]
  \begin{block}{The Focus}
    What can we do with panel data to mitigate bias, which is not possible with cross-sectional data?
  \end{block}
  \begin{block}{As it will turn out,}
    Panel data can be very powerful compared to cross-sectional data
  \end{block}
\end{frame}

\begin{frame}[c]
  \frametitle{Data type}
  \begin{block}{Independently pooled cross sectional (IPCS) data}
    Data obtained by sampling randomly from a large population at different points in time without deliberate effort to collect observations of the same cross-sectional units across time
    \begin{itemize}
      \item sample of hourly wage, education, experience, and so on from the population of working people each year
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \frametitle{Data type}
  \begin{block}{Panel (longitudinal) data}
    Data follow the \textcolor{blue}{same} individuals, families, firms, cities, states or whatever, across time
    \begin{itemize}
      \item randomly selecting people from a population at a given point in time.
      \item then the same people are reinterviewed at several subsequent points in time, which would results in data on wages, hours, education, and so on, for the same group of people in different years.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \frametitle{Panel Data}
<<sample_panel, echo=FALSE, cache=TRUE>>=
  data_tr <- readRDS('jtrain.rds') # load the data
  dplyr::select(data_tr,year,fcode,employ,sales) %>%
    head(9)
@
\end{frame}

\begin{frame}[c]
  \begin{block}{New way of writing a model}
  \vspace{-0.4cm}
  \begin{align*}
    y_{i,t} = \beta_0 + \beta_1 x_{1,i,t} + \beta_2 x_{2,i,t} + \dots \beta_k x_{k,i,t} + u_{i,t}
  \end{align*}
  \vspace{-0.6cm}
  \begin{itemize}
      \item $i$: indicates cross-sectional unit
      \item $t$: indicates time
    \end{itemize}
  \end{block}
  \begin{block}{Pooled OLS estimation}
    Proceed exactly the same way as before with OLS on a cross-sectional dataset
  \end{block}
\end{frame}

\begin{frame}[c]
  \frametitle{Analysis of IPCS data}
  \begin{itemize}
    \item Analytical framework and method we have used can be used: there is not much you can do differently than with cross-sectional data
    \item One reason for using IPCS is to increase sample size for estimation accuracy
    \item It can be used to do \textcolor{blue}{program (impact) evaluation} more credibly than using cross-sectional data
  \end{itemize}
\end{frame}

\begin{frame}[c]
  \frametitle{Program evaluation (policy analysis)}
  \begin{block}{Randomized experiment}
    Changes in the environment in which agents operate, which are assigned randomly to agents \textcolor{blue}{by the investigators}
    \begin{itemize}
      \item financial aid to start up a new business provided randomly to some people
      \item Randomly change school/teach ratio to measure its impact on education quality
    \end{itemize}
  \end{block}
  \only<2->{\begin{block}{Evaluation of a randomized experiment}
    Great! Not really much to worry about!
    \begin{align*}
      y \;\;(\mbox{income}) = \beta_0 + \beta_1 program \;\;(\mbox{financial aid}) + u
    \end{align*}
    , where $E[u|program]=0$. \textcolor{red}{OLS} is just fine.
  \end{block}}
\end{frame}

\begin{frame}[t]
  \frametitle{Program evaluation (policy analysis)}
  \begin{block}{Natural Experiment (Quasi-experiment)}
    An event or policy change (often a change in government policy) that happens \textcolor{blue}{outside of the control of investigators}, which changes the environment in which agents (individuals, families, firms, or cities) operate.
    \begin{itemize}
      \item availability of livestock insurance program
      \item job training program
      \item water use limits
    \end{itemize}
  \end{block}
  \only<2->{\begin{block}{Evaluation of a natural experiment}
    Gotta be more careful!
    \begin{align*}
      y \;\;(\mbox{land price}) = \beta_0 + \beta_1 program \;\;(\mbox{livestock insurance}) + u
    \end{align*}
    \only<3->{Is $E[u|program]=0$?}
  \end{block}}
\end{frame}

\begin{frame}[c]
  \begin{block}{Evaluation of a natural experiment using IPCS data}
    If you have IPCS data, instead of cross-sectional data, you may be able to evaluate the impact of a program more credibly!
  \end{block}

  \begin{block}{Natural experiment data structure}
    Observations on two groups before and after the program placement:
    \begin{itemize}
      \item Treated (treated at $t=2$)
      \begin{itemize}
        \item observed at $t=1$ (before)
        \item observed at $t=2$ (after)
      \end{itemize}
      \item Control (untreated)
      \begin{itemize}
        \item observed at $t=1$ (before)
        \item observed at $t=2$ (after)
      \end{itemize}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \frametitle{Difference-in-Differences (DID) Estimation}
  \begin{itemize}
     \item DID can be conducted using either IPCS or panel data, but not using cross-sectional data
     \item DID can be a very useful strategy to estimate the impact of policy changes
  \end{itemize}
\end{frame}

\begin{frame}[c]
  \frametitle{DID by example: The impact of incinerator}
  \begin{block}{Incinerator construction}
    \begin{itemize}
      \item rumored about the incinerator being built in North Andover, Massachusetts, began in 1978
      \item construction started in 1981
    \end{itemize}
  \end{block}
  \begin{block}{Data collected}
      Housing prices in 1978 and 1981, and other variables
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{Naive analysis (sort of what you did in assignment 1)}
  Run regression on the following model using the 1981 data (cross-sectional data)
  \begin{align*}
    rpice = \gamma_0 + \gamma_1 nearinc + u
  \end{align*}
  \vspace{-0.6cm}
  \begin{itemize}
    \item $rprice$: house price in real terms (inflation-corrected)
    \item $nearinc$: 1 if the house is near the incinerator, and 0 otherwise
    \item \textcolor{blue}{$\gamma_1$: the difference between the mean house price of houses nearby the incinerator and the rest (not nearby) in 1981}
  \end{itemize}
  \end{block}

  \only<2->{\begin{block}{What is the problem with this approach?}
    Is $nearinc$ endogenous?
  \end{block}}
\end{frame}

\begin{frame}[c,fragile]

\begin{block_code}{R code: OLS using the 1981 data}
<<data_import, cache=TRUE>>=
  data <- read.dta13('KIELMC.dta') %>%
    mutate(rprice=rprice/1000)
  reg_81 <-lm(rprice~nearinc,data=filter(data,year==1981))
@
\end{block_code}

<<naive_81, cache=TRUE, echo=FALSE, results='asis'>>=
  stargazer(reg_81,type='latex',single.row=TRUE,omit.stat=c('all'),table.layout='-ld#-t-')
@
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Naive analysis}
  Run regression on the following model using the \textcolor{blue}{1978} data
  \begin{align*}
    rpice = \lambda_0 + \lambda_1 nearinc + u
  \end{align*}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
\begin{block_code}{R code: OLS using the 1978 data}
<<reg_78, cache=TRUE>>=
  reg_78 <-lm(rprice~nearinc,data=filter(data,year==1978))
@
\end{block_code}

<<naive_78, cache=TRUE, echo=FALSE, results='asis'>>=
  stargazer(reg_78,type='latex',single.row=TRUE,omit.stat=c('all'),table.layout='-ld#-t-')
@

\begin{block}{So,}
  Houses nearby the incinerator were already lower than those houses that are not nearby...
\end{block}
\end{frame}


\begin{frame}[c,fragile]

\begin{block_code}{R code: Visualization}
<<did, cache=TRUE>>=
  data_mean <- data %>%
    group_by(year,nearinc) %>%
    summarize(m_rprice=mean(rprice))

  g_dif <- ggplot(data=data_mean) +
    geom_bar(aes(y=m_rprice,x=factor(year),fill=factor(nearinc)),
    stat='identity',position='dodge') +
    ylab('Mean House Price ($1000$)') +
    xlab('Year') +
    scale_fill_discrete(name='') +
    theme(
      legend.position='bottom'
    )
@
\end{block_code}
\end{frame}

\begin{frame}[c,fragile]
<<did_viz, cache=TRUE, echo=FALSE,dependson='did',out.height='3.5in',out.width='3.5in'>>=
  g_dif
@
\end{frame}

\begin{frame}[c]

\only<1-1>{\begin{block}{Estimated Model}
\vspace{-0.6cm}
  \begin{align*}
    \mbox{(1981)}\;\; rpice & = \gamma_0 + \gamma_1 nearinc + u \\
    \mbox{(1978)}\;\; rpice & = \lambda_0 + \lambda_1 nearinc + u
  \end{align*}
\end{block}}

\only<1-2>{\begin{block}{According to the models,}
\vspace{-0.6cm}
  \begin{align*}
    E[rprice|year=1981,nearinc=0] & = \gamma_0 \\
    E[rprice|year=1981,nearinc=1] & = \gamma_0 + \gamma_1 \\
    E[rprice|year=1978,nearinc=0] & = \lambda_0 \\
    E[rprice|year=1978,nearinc=1] & = \lambda_0 + \lambda_1
  \end{align*}
\end{block}}

\only<2-3>{\begin{block}{Interpretation}
\vspace{-0.6cm}
  \begin{align*}
    \gamma_1 = & E[rprice|year=1981,nearinc=1]  \\
      & - E[rprice|year=1981,nearinc=0] \\
    \lambda_1 = & E[rprice|year=1978,nearinc=1]  \\
     & - E[rprice|year=1978,nearinc=0]
  \end{align*}
\end{block}}

\only<3-4>{\begin{block}{DID}
\vspace{-0.6cm}
\begin{align*}
  DID  = & \gamma_1 - \lambda_1  \\
       = & \Big(E[rprice|year=1981,nearinc=1]  \\
         & - E[rprice|year=1978,nearinc=1]\Big)  \\
         &    - \Big(E[rprice|year=1981,nearinc=0] \\
         & - E[rprice|year=1978,nearinc=0]\Big)
\end{align*}
\end{block}}
\end{frame}

\begin{frame}[c,fragile]
<<did_viz_2, echo=FALSE,out.height='3.5in',out.width='3.5in'>>=
  g_dif
@
\end{frame}

\begin{frame}[c]


\begin{center}
\begin{table}
\caption{Expected House Price}
\begin{tabular}{ccc}
  & before & after \\\hline
 nearinc=0 & $\gamma_0$ & $\gamma_0$ + $\alpha_0$ + 0\\
 nearinc=1 & $\gamma_1$ & $\gamma_1$ + $\alpha_1$ + $\beta$ \\\hline
\end{tabular}
\end{table}
\end{center}
\only<1-1>{\begin{itemize}
  \item $\gamma_j$ is the expected house price of those that are $nearinc=j$
  \item $\alpha_j$ is \textcolor{blue}{any} macro shocks \textcolor{blue}{other than the incinerator event} that happened between the before and after period to the houses that are $nearinc=j$
  \item $\beta$ is the true causal impact of the incinerator placement
\end{itemize}}

\only<2-2>{
  \footnotesize{\begin{block}{OLS on the 1981 data}
  \vspace{-0.4cm}
  \begin{align*}
    E[\hat{\beta}] = & E[rpice|nearinc==1,after]\\
    & -E[rpice|nearinc==0,after]\\
    = &  (\gamma_1 + \alpha_1 + \beta) - (\gamma_0 + \alpha_0)\\
    = &  [(\gamma_1-\gamma_0) + (\alpha_1-\alpha_0)] + \beta
  \end{align*}
  \end{block}
  \begin{block}{Bias}
  \vspace{-0.4cm}
  \begin{align*}
    E[bias]=[(\gamma_1-\gamma_0) + (\alpha_1-\alpha_0)]
  \end{align*}
  \end{block}}
}

\only<3-3>{
  \footnotesize{\begin{block}{OLS on the IPCS data without the control group} \vspace{-0.4cm}
  \begin{align*}
    E[\hat{\beta}] = & E[rpice|nearinc==1,after]\\
    & -E[rpice|nearinc==1,before]\\
    = &  (\gamma_1 + \alpha_1 + \beta) - (\gamma_1)\\
    = &  [\alpha_1] + \beta
  \end{align*}
  \end{block}

  \begin{block}{Bias}
  \vspace{-0.4cm}
  \begin{align*}
    E[bias]=\alpha_1
  \end{align*}
  \end{block}}
}

\only<4-4>{
\footnotesize{\begin{block}{DID}
  \vspace{-0.4cm}

  \begin{align*}
    E[\hat{\beta}] = & \Big(E[rprice|year=1981,nearinc=1]  \\
         & - E[rprice|year=1978,nearinc=1]\Big)  \\
         &    - \Big(E[rprice|year=1981,nearinc=0] \\
         & - E[rprice|year=1978,nearinc=0]\Big) \\
         & = \Big((\gamma_1 + \alpha_1 + \beta) - (\gamma_1)\Big)-\Big((\gamma_0 + \alpha_0 + \beta) - (\gamma_0)\Big)\\
         & = (\alpha_1+\beta) -(\alpha_0) = (\alpha_1-\alpha_0)+\beta
  \end{align*}
  \end{block}

  \begin{block}{Bias}
  \vspace{-0.4cm}
  \begin{align*}
    E[bias]=\alpha_1-\alpha_0
  \end{align*}
  \end{block}
}
}

\end{frame}

\begin{frame}[c]
  \begin{block}{Condition under which DID works (Unbiased)}
  \vspace{-0.4cm}
  \begin{align*}
    \alpha_1=\alpha_0
  \end{align*}
    The dependent variable of the treatment ($nearinc=1$) group would have grown by the same amount as the control group ($nearinc=0$) if it were not for the treatment (incinerator construction)
  \end{block}

  \begin{block}{Important}
    \begin{itemize}
      \item The above condition is \textcolor{blue}{untestable}
      \item If there were any significant changes in policy that happen to only one of them other than the treatment of interest, DID confound the impact of the other policy and the treatment of interest
      \item It is a common practice to present the trend of the dependent variable prior to the year before the treatment happened
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
\begin{block}{DID in single regression}
  \begin{align*}
    rprice=\beta_0 + \sigma_0 y81 + \beta_1 nearinc + \sigma_1 y81\cdot nearinc+ u
  \end{align*}
  where $\hat{\sigma_1}$ is the DID estimate. You can include other variables as controls.
\end{block}
\end{frame}



\begin{frame}[c]
  \title{Two-Period Panel Data Analysis}
  \author{}
  \date{}
  \maketitle
\end{frame}

\begin{frame}[c]
  \frametitle{}
  \begin{block}{Panel (longitudinal) data}
    Data follow the \textcolor{blue}{same} individuals, families, firms, cities, states or whatever, across time
    \begin{itemize}
      \item randomly selecting people from a population at a given point in time.
      \item then the same people are reinterviewed at several subsequent points in time, which would results in data on wages, hours, education, and so on, for the same group of people in different years.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{Two-period panel data}
    For a cross section of individuals, schools, firms, cities, or whatever, we have two years of data; call these $t=1$ and $t=2$. These years need not be adjacent, but $t=1$ corresponds to the earlier year.
  \end{block}
  \begin{block}{An example we use}
    Crime and unemployment rates data
    \begin{itemize}
      \item 46 cities (cross-sectional unit is city)
      \item $1982$ ($t=1$) and $1987$ ($t=2$)
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{OLS }
    Regression analysis using only the $1987$ data (cross-sectional);
  \begin{align*}
  crmrte = \beta_0 + \beta_1 unem + u
  \end{align*}
  \end{block}

  \begin{block_code}{R code: Crime Data}
<<fe_crime, cache=TRUE>>=
  data <- read.dta13('CRIME2.dta')
  reg_87 <-lm(crmrte~unem,data=filter(data,year==87))
@
  \end{block_code}

<<naive_87, cache=TRUE, echo=FALSE, results='asis'>>=
  stargazer(reg_87,type='latex',single.row=TRUE,omit.stat=c('all'),table.layout='-ld#-t-')
@

\end{frame}

\begin{frame}[c]

<<naive_87_2, cache=TRUE, echo=FALSE, results='asis'>>=
  stargazer(reg_87,type='latex',single.row=TRUE,omit.stat=c('all'),table.layout='-ld#-t-')
@

\begin{block}{Question}
  \begin{itemize}
    \item Is the negative sign of the coefficient estimate on $unem$ what you expected?
    \item Any problem with this regression?
      \begin{itemize}
        \item we could include other controls like age distribution, gender distribution, education levels, law enforcement efforts, etc
        \item there could be many other factors that cannot be observed
      \end{itemize}
  \end{itemize}
\end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{Taking advantage of the panel data structure}
    You can take advantage of the panel data structure to mitigate the omitted variable bias
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{View the unobserved factors differently}
  \vspace{-0.6cm}
    \begin{align*}
    y_{i,t} = \beta_0 + \sigma_0 d2_t + \beta_1 x_{i,t} + v_{i,t}
    \end{align*}
    \vspace{-0.6cm}
    \begin{itemize}
      \item subscripts $i$ and $t$ indicate cross-sectional unit (city) and time (year), respectively
      \item $d2_t$: 0 when $t=1$ and 1 when $t=2$ (no $i$ subscript because it does not change across $i$)
    \end{itemize}
  \end{block}

  \begin{block}{The error term: $v_{i,t}$}
  The error term consists of two parts: $\alpha_i$ and $u_{i,t}$
  \begin{align*}
    v_{i,t}=\alpha_i+u_{i,t}
  \end{align*}
  \vspace{-0.6cm}
  \begin{itemize}
     \item $\alpha_i$: all unobserved, \textcolor{blue}{time-invariant} factors of $i$ that affect $y_{i,t}$ (referred to as \textcolor{blue}{fixed effects} and \textcolor{blue}{unobserved heterogeneity})
     \item $u_{i,t}$: the rest of the error that is time-varying (often referred to as \textcolor{blue}{idiosyncratic error})
   \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{Fixed (Unobserved) Effects Model Example}
  \vspace{-0.6cm}
    \begin{align*}
    crmrte_{i,t} = \beta_0 + \sigma_0 d87_t + \beta_1 unem_{i,t} + \alpha_i + u_{i,t}
    \end{align*}
    \vspace{-0.6cm}
    \begin{itemize}
      \item Since $i$ denotes different cities, we call $\alpha_i$ an \textcolor{blue}{unobserved city effect} or a \textcolor{blue}{city fixed effect}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{city fixed effect: $\alpha_i$}
  \begin{itemize}
    \item It represents \textcolor{blue}{all} factors affecting city crime rates that \textcolor{blue}{do not change over time}. (Geographical features, such as the cityâ€™s location in the United States, are included in $\alpha_i$)
    \item Many other factors may not be exactly constant, but they might be roughly constant over a five-year period (slow to change)
      \begin{itemize}
        \item certain demographic features of the population (age, race, and education)
        \item different cities may have their own methods for reporting crimes
        \item the people living in the cities might have different attitudes toward crime
      \end{itemize}
  \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{Estimation of the model: Pooled OLS (POLS)}
  Pools the data and run OLS on the following model
    \begin{align*}
    y_{i,t} = \beta_0 + \sigma_0 d2_t + \beta_1 x_{i,t} + v_{i,t}
    \end{align*}
  where $v_{i,t}=\alpha_i+u_{i,t}$
  \end{block}
  \begin{block}{Condition under which POLS is unbiased}
  \begin{align*}
    E[v_{i,t}|x_{i,t}]&=0 \\
    \Rightarrow E[u_{i,t}|x_{i,t}]&=0 \;\;\mbox{and}\;\; E[\alpha_i|x_{i,t}]=0\\
  \end{align*}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: Crime Data}
<<pols_crime, cache=TRUE>>=
  data <- read.dta13('CRIME2.dta')
  reg_pols <-lm(crmrte~unem+d87,data=data)
@
  \end{block_code}

<<pols, cache=TRUE, echo=FALSE, results='asis', dependson='pols_crime'>>=
  stargazer(reg_pols,type='latex',single.row=TRUE,omit.stat=c('all'),table.layout='-ld#-t-')
@

\end{frame}

\begin{frame}[c]
  \frametitle{Taking advantage of the panel structure}
  \begin{block}{The Idea}
    \textcolor{blue}{Difference $\alpha_i$ out!!}
    \begin{align*}
      (t=2) \;\;& y_{i,2} = \beta_0 + \sigma_0 + \beta_1 x_{i,2} + a_i + u_{i,2}\\
      (t=1) \;\;& y_{i,1} = \beta_0 + \beta_1 x_{i,1} + a_i + u_{i,1}
    \end{align*}
    Subtracting the second from the first,
    \begin{align*}
      (y_{i,2}-y_{i,1})&=\sigma_0 + \beta_1 (x_{i,2}-x_{i,1}) + (u_{i,2}-u_{i,1}) \;\; \mbox{or}\\
      \Delta y_i&=\sigma_0 + \beta_1 \Delta x_{i} + \Delta u_{i}
    \end{align*}
    We call this \textcolor{blue}{first-differenced equation}.
  \end{block}
\end{frame}

\begin{frame}[t]
\only<1-2>{\begin{block}{First-differenced (FD) estimator}
  Run OLS on the first differenced model:
  \begin{align*}
      \Delta y_i&=\sigma_0 + \beta_1 \Delta x_{i} + \Delta u_{i}
  \end{align*}
\end{block}}

\only<2-3>{\begin{block}{\textcolor{blue}{Strict Exogeneity}: conditions for unbiasedness}
\vspace{-0.6cm}
\begin{align*}
  E[\Delta u_{i,t}|\Delta x_{i,t}] = 0,
\end{align*}
which is satisfied if
\begin{align*}
  E[u_{i,t}|x_{i,s}] = 0 \;\; ^{\forall}s \;\;\mbox{and} \;\;t,
\end{align*}
meaning
\begin{align*}
  E[u_{i,1}|x_{i,1}] & = 0\\
  E[u_{i,2}|x_{i,1}] & = 0\\
  E[u_{i,1}|x_{i,2}] & = 0\\
  E[u_{i,2}|x_{i,2}] & = 0
\end{align*}
\end{block}}

\only<3->{\begin{block}{Important}
  It is \textcolor{red}{okay} that $x_{i,t}$ is correlated with $\alpha_i$ unlike POLS (we sometimes say correlation between $x_{i,t}$ with $\alpha_i$ is allowed).
\end{block}}
\end{frame}

\begin{frame}[c,fragile]
  \frametitle{Implementation of FD estimation in R}
  \begin{block}{$plm$ package}
    \begin{itemize}
      \item $pdata.frame()$: create a data.frame that is aware of the cross-sectional units and time
      \item $plm$: implement a variety of panel data estimation methods
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: Crime Data}

<<fd_implement>>=
  #--- library plm ---#
  library(plm)

  #--- take a look at the portion of the data ---#
  data %>% dplyr::select(crmrte,unem,year) %>% head()

  dplyr::select(data,city_id,crmrte,unem,year)

  #--- create city (cross-sectional unit) id ---#
  # you normally do not have to do this
  data <- mutate(data,city_id=rep(seq(1,46),each=2))

  #--- convert the dataset ---#
  pdata_crm <- pdata.frame(data,index=c('city_id','year'))
@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: FD estimation}

<<fd_implement_plm, cache=TRUE, size='scriptsize'>>=
  #--- FD estimation ---#
  reg_fd <- plm(crmrte~unem,data=pdata_crm,model='fd')

  #--- summary of the estimation ---#
  summary(reg_fd)$coef
@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]

\begin{block_code}{R code: stargaze the FD and POLS results}
<<stg_plm, eval=FALSE, cache=TRUE>>=
  #--- stargazer ---#
  stargazer(reg_fd,reg_pols,type='latex',single.row=TRUE,
  omit.stat=c('all'),table.layout='-ldc-t-o-n',
  column.labels=c('FD','POLS'),omit='Constant',
  omit.labels='Intercept?',omit.yes.no=c('Yes','Yes'))
@
\end{block_code}

<<stg_plm_disp,echo=FALSE, cache=TRUE, results='asis'>>=
  stargazer(reg_fd,reg_pols,type='latex',single.row=TRUE,omit.stat=c('all'),table.layout='-ldc-t-o-n', column.labels=c('FD','POLS'),omit='Constant',omit.labels='Intercept?',omit.yes.no=c('Yes','Yes'))
@

\end{frame}

\begin{frame}[c]
  \begin{block}{Some Drawbacks}
    \begin{itemize}
      \item time-constant variables are dropped by first-differencing
      \item you typically lose variation in explanatory variables (sometimes substantially)
        \begin{itemize}
          \item residential water price
          \item education
        \end{itemize}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
   \begin{block}{Another example}
   \vspace{-0.6cm}
     \begin{align*}
       log(wage)_{i,1} & =\beta_0+\beta_1 educ_{i,1} + \beta_2 ability_i + u_{i,1} \\
       log(wage)_{i,2} & =\beta_0+\beta_1 educ_{i,2} + \beta_2 ability_i + u_{i,2}
     \end{align*}
   \end{block}
   \begin{block}{First-differencing}
   \vspace{-0.6cm}
   \begin{align*}
     \Delta log(wage)_i = \beta_1 \Delta educ_i + \Delta u_i
   \end{align*}
   \end{block}
   \begin{block}{Note}
   \begin{itemize}
     \item So, as long as ability is indeed time-invariant, you do not have to worry about correlation between education and ability
     \item Even if ability is time-variant, it should change very slowly over time. Consequently, most of the correlation between ability and education should be gone.
   \end{itemize}
   \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{Program (impact) evaluation with two-period panel data}
  \begin{itemize}
    \item Panel data sets are very useful for policy analysis and, in particular, \textcolor{blue}{program evaluation}
    \item Note that the important difference from the incinerator example is that the same cross-sectional units appear in each time period
  \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{Example: Michigan job training program}
    \begin{itemize}
      \item improve the worker productivity of manufacturing firms
      \item grants were awarded on a first-come, first-served basis
      \item you are interested in evaluating the impact of this program on products scrap rate
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{A Model}
    \begin{align*}
      log(scrap)_{i,t} = \beta_0 + \sigma_0 y88_t + \beta_1 grant_{i,t} + \alpha_i + u_{i,t}
    \end{align*}
    \begin{itemize}
      \item $scrap_{i,t}$: products scrap rate for firm $i$ at time $t$
      \item $y88_t$: year dummy that takes 1 if in 1988, 0 if in 1987
      \item $grant_{i,t}$: 1 if you received a grant to have workers participaite in the program at $t$, 0 otherwise.
      \item $\alpha_i$: unobserved time-invariant firm characteristics
      \item $u_{i,t}$ idiosyncratic error
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{Question}
    What would be in $\alpha_i$?
    \begin{itemize}
      \item \only<2->{employee ability}
      \item \only<3->{capital}
      \item \only<4->{management skill}
    \end{itemize}
    \only<5->{These factors should be slow to change in the 2-year period}
  \end{block}
  \only<6->{\begin{block}{Question}
    Is $grant_{i,t}$ endogenous? In other words, is $grant_{i,t}$ correlated with any of the above firm characteristics? (\textcolor{blue}{pay attention to the way the grants are granted!})
  \end{block}}
  \only<7->{\begin{block}{Answer}
    Likely. Why?
  \end{block}}
\end{frame}

\begin{frame}[c]
  \begin{block}{A model}
  \vspace{-0.6cm}
    \begin{align*}
      log(scrap_{i,t}) = \beta_0 + \sigma_0 y88_t + \beta_1 grant_{i,t} + \alpha_i + u_{i,t}
    \end{align*}
  \end{block}

  \begin{block}{two-periods}
  \vspace{-0.6cm}
  \begin{align*}
    t=2:\;\; log(scrap_{i,2}) = & \beta_0 + \sigma_0 y88_t (=1) \\
      & + \beta_1 grant_{i,2} + \alpha_i + u_{i,2} \\
    t=1:\;\; log(scrap_{i,1}) = & \beta_0 + \sigma_0 y88_t (=0) \\
      & + \beta_1 grant_{i,1} + \alpha_i + u_{i,1}
  \end{align*}
  \end{block}

  \begin{block}{First-differencing}
  \vspace{-0.6cm}
  \begin{align*}
    \Delta log(scrap)_{i} = \sigma_0 + \beta_1 \Delta grant_{i} + \Delta u_{i}
  \end{align*}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: POLS and FD Estimation}
<<train, cache=TRUE, results='hide'>>=
  #--- preparation ---#
  data_tr <- readRDS('jtrain.rds') %>% # load the data
    mutate(avgsal=avgsal/1000) %>%
    filter(year!=1989)

  #--- POLS ---#
  reg_pols <- lm(log(scrap)~grant+d88,data=data_tr)

  #--- FD ---#
  pdata_tr <- pdata.frame(
    data_tr,
    index=c('fcode','year')
    )
  reg_fd <- plm(log(scrap)~grant,data=pdata_tr,model='fd')
@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
<<train_st, cache=TRUE, eval=FALSE>>=
  #--- stargaze ---#
  stargazer(reg_fd,reg_pols,type='latex',
    single.row=TRUE,omit.stat=c('all'),table.layout='-ldc-t-o-n',
    column.labels=c('FD','POLS'),omit='Constant',
    omit.labels='Intercept?',omit.yes.no=c('Yes','Yes'))
@

<<train_st_2, cache=TRUE, echo=FALSE, results='asis'>>=
  stargazer(reg_fd,reg_pols,type='latex',
    single.row=TRUE,omit.stat=c('all'),table.layout='-ldc-t-o-n',
    column.labels=c('FD','POLS'),omit='Constant',
    omit.labels='Intercept?',omit.yes.no=c('Yes','Yes'))
@
\end{frame}

\begin{frame}[c]
<<train_st_3, cache=TRUE, echo=FALSE, results='asis'>>=
  stargazer(reg_fd,reg_pols,type='latex',
    single.row=TRUE,omit.stat=c('all'),table.layout='-ldc-t-o-n',
    column.labels=c('FD','POLS'),omit='Constant',
    omit.labels='Intercept?',omit.yes.no=c('Yes','Yes'))
@
  \begin{block}{Bias direction}
      What seems to be the sign of the bias of not controlling for unobserved firm characteristics?
  \end{block}
\end{frame}


\begin{frame}[c]
  \only<1-2>{\begin{block}{General Framework}
  \vspace{-0.6cm}
    \begin{align*}
      y_{i,t}=\beta_0+\sigma_0 d2_t + \beta_1 prog_{i,t} + \alpha_i + u_{i,t}
    \end{align*}
  \end{block}}
  \only<2-3>{\begin{block}{First-Differencing}
  \vspace{-0.6cm}
    \begin{align*}
      \Delta y_{i}=\sigma_0 + \beta_1 \Delta prog_{i} + \Delta u_{i}
    \end{align*}
  \end{block}}
  \only<3-4>{\begin{block}{Interpretation of $\beta_1$}
  \vspace{-0.6cm}
  \begin{align*}
    E[\Delta y_{i}|treated] & =\sigma_0 + \beta_1 \\
    E[\Delta y_{i}|control] & =\sigma_0
  \end{align*}
  So,
  \begin{align*}
    \beta_1 = E[\Delta y_{i}|treated] - E[\Delta y_{i}|control]
  \end{align*}
  \end{block}}
  \only<4-5>{\begin{block}{DID}
    FD estimator is DID for two-period panel data
  \end{block}}
  \only<5->{\begin{block}{Adding other controls}
    \begin{align*}
      y_{i,t}= & \beta_0+\sigma_0 d2_t + \beta_1 prog_{i,t} \\
      & + \textcolor{blue}{\alpha_1 x1_{i,t} + \dots + \alpha_k xk_{i,t}} + \alpha_i + u_{i,t}
    \end{align*}
    where $x1,\dots,xk$ are other controls
  \end{block}}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: FD Estimation with other controls}
<<train_wc, cache=TRUE>>=
  #--- FD ---#
  reg_fd <- plm(log(scrap)~grant+avgsal+tothrs,
    data=pdata_tr,model='fd')

  #--- summary ---#
  summary(reg_fd)$coef
@
  \end{block_code}
\end{frame}

\begin{frame}[c]
  \title{Fixed Effects (FE) and Random Effects (RE) estimation methods}
  \author{}
  \date{}
  \maketitle
\end{frame}

\begin{frame}[c]
  \only<1-1>{\begin{block}{Idea}
    First-difference is not the only data transformation that eliminate individual fixed effects (time-invariant individual characteristics)
  \end{block}}
  \only<2->{\begin{block}{Within-transformation}
    Consider the following general model:
    \begin{align*}
      y_{i,t}=\beta_1 x_{i,t} + \alpha_i + u_{i,t}
    \end{align*}
    For each $i$, average this equation over time, we get
    \begin{align*}
      \frac{\sum_{t=1}^T y_{i,t}}{T} = \frac{\sum_{t=1}^T x_{i,t}}{T} + \alpha_i \;\; (\frac{\sum_{t=1}^T \alpha_{i}}{T}) + \frac{\sum_{t=1}^T u_{i,t}}{T}
    \end{align*}
    Subtracting the second equation from the first one,
    \begin{align*}
      (y_{i,t}-\frac{\sum_{t=1}^T y_{i,t}}{T})=\beta_1 (x_{i,t} -\frac{\sum_{t=1}^T x_{i,t}}{T}) + (u_{i,t} -\frac{\sum_{t=1}^T u_{i,t}}{T})
    \end{align*}
  \end{block}}
\end{frame}

\begin{frame}[c]
  \begin{block}{Within-transformation}
    \begin{align*}
      (y_{i,t}-\frac{\sum_{t=1}^T y_{i,t}}{T})=\beta_1 (x_{i,t} -\frac{\sum_{t=1}^T x_{i,t}}{T}) + (u_{i,t} -\frac{\sum_{t=1}^T u_{i,t}}{T})
    \end{align*}
   Alternatively, we sometimes write as follows (at least the book does):
   \begin{align*}
     \ddot{y}_{i,t}=\beta_1 \ddot{x}_{i,t} + \ddot{u}_{i,t}
   \end{align*}
   \end{block}
   \begin{block}{Important}
     $\alpha_i$ is gone!!
   \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: Within-transformed data}
<<raw_data, echo=FALSE, cache=TRUE>>=
  raw_data <- data.table(
    id = rep(1:2,each=3),
    year = rep(2015:2017,2),
    income = c(77,82,84,110,125,131),
    educ = c(12:14,18:20)
  )
  raw_data[,mean_income:=mean(income),by=id]
  raw_data[,mean_educ:=mean(educ),by=id]
  raw_data[,wt_income:=income-mean_income]
  raw_data[,wt_educ:=educ-mean_educ]
  raw_data
@
  \end{block_code}
\end{frame}

\begin{frame}[c]
\begin{block}{Model with multiple variables in general}
\vspace{-0.6cm}
  \begin{align*}
  y_{i,t} = \beta_0 + \beta_1 x_{1,i,t} + \beta_2 x_{2,i,t} + \dots + \beta_k x_{k,i,t} + \alpha_i + u_{i,t}
  \end{align*}
\end{block}
\begin{block}{Within-transformed model}
\vspace{-0.6cm}
\begin{align*}
  \ddot{y}_{i,t} = \beta_0 + \beta_1 \ddot{x}_{1,i,t} + \beta_2 \ddot{x}_{2,i,t} + \dots + \beta_k \ddot{x}_{k,i,t} + \ddot{u}_{i,t}
\end{align*}
\end{block}

\end{frame}

\begin{frame}[c]
  \only<1-2>{\begin{block}{Fixed Effect Estimation}
    Run OLS on the within-transformed model:
\begin{align*}
  \ddot{y}_{i,t} = \beta_0 + \beta_1 \ddot{x}_{1,i,t} + \beta_2 \ddot{x}_{2,i,t} + \dots + \beta_k \ddot{x}_{k,i,t} + \ddot{u}_{i,t}
\end{align*}
  \end{block}}

  \only<2-3>{\begin{block}{Conditions under which FE is unbiased} \vspace{-0.6cm}
  \begin{align*}
    E[\ddot{u}_{i,t}|\ddot{x}_{j,i,t}] = 0 \;\; ^\forall j=1,\dots,k
  \end{align*}
  , which is satisfied if
  \begin{align*}
    E[u_{i,s}|x_{j,i,t}] = 0 \;\; ^\forall s, \;\; t, \;\;\mbox{and} \;\;j
  \end{align*}
  (ex. $E[\ddot{u}_{i,1}|\ddot{x}_{1,i,2}]=0$)
  \end{block}}

  \only<3-4>{\begin{block}{Note}
  \begin{itemize}
    \item If an independent variable is a function of the unobserved factors in the previous period, the above condition is violated
    \item If an independent variable is a function of the dependent variable (, which includes the error term) in the previous period, the above condition is violated
  \end{itemize}
  \end{block}}

  \only<4->{
  \begin{block}{Example}
  \vspace{-0.6cm}
  \begin{align*}
    crmrte_{i,t} = \beta_0 + \beta_1 lawenf + \alpha_i + u_{i,t}
  \end{align*}
  \vspace{-0.6cm}
  \begin{itemize}
    \item $crmrte$: crime rate
    \item $lawenf$: law enforcement efforts
  \end{itemize}
  \end{block}
  }

  \only<5->{
  \begin{block}{Question}
    Would $lawenf$ be a function of $crmrte$ in the previous periods?
  \end{block}
  }

\end{frame}

\begin{frame}[c,fragile]
  \frametitle{Implementation of FE estimation in R}
  \begin{block}{Model}
  \vspace{-0.6cm}
  \begin{align*}
    log(scrap_{i,t}) & = \beta_0 + \beta_1 grant_{i,t} + \beta_2 avgsal_{i,t} + \beta_3 tothrs_{i,t} \\
    & + \beta_4 union_{i,t} + \alpha_i + u_{i,t}
  \end{align*}
  \end{block}
  \begin{block_code}{R code: FE estimation}

<<FE_est, cache=TRUE,size='scriptsize',dependson='train'>>=
  #--- create pdata.frame ---#
  pdata_tr <- pdata.frame(data_tr,index=c('fcode','year'))

  #--- FE estimation ---#
  # just tell R that your method is 'within'
  reg_fe <- plm(log(scrap)~grant+avgsal+tothrs,
    data=pdata_tr,model='within')

  #--- summary of the estimation ---#
  summary(reg_fe)$coef
@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Least square dummy variable (LSDV) regression}
    \begin{itemize}
      \item Instead of within-transforming the model, include individual dummy variable and run OLS (We call this \textcolor{blue}{least square dummy variable regression}).
      \item It turns out FE estimation is nothing but LSDV (they are mathematically equivalent)
    \end{itemize}
  \end{block}

  \begin{block_code}{R code: Example}
<<raw_data_lsdv, echo=FALSE, cache=TRUE>>=
  raw_data <- data.table(
    id = rep(1:2,each=3),
    year = rep(2015:2017,2),
    income = c(77,82,84,110,125,131),
    educ = c(12:14,18:20)
  )
  raw_data[,fe_1:=0]
  raw_data[id==1,fe_1:=1]
  raw_data[,fe_2:=0]
  raw_data[id==2,fe_2:=1]
  raw_data
@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{FE and LSDV}
    They will give you identical coefficient and standard error estimates
  \end{block}

  \begin{block_code}{R code: LSDV vs FE}
<<lsdv_fe, cache=TRUE>>=
    #--- lsdv ---#
    reg_lsdv_ex <- lm(income~educ+fe_1+fe_2,data=raw_data)

    #--- fe ---#
    pdata <- pdata.frame(raw_data,index=c('id','year'))
    reg_fe_ex <- plm(income~educ,data=pdata)
@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: stargazer}
<<lsdv_fe_str, eval=FALSE, cache=TRUE, size='scriptsize'>>=
  stargazer(reg_lsdv_ex,reg_fe_ex,type='latex',
    single.row=TRUE,omit.stat=c('all'),table.layout='-ldc-t-o-n',
    column.labels=c('LSDV','FE'),omit='Constant',
    omit.labels='Intercept?',omit.yes.no=c('Yes','Yes'))
@
  \end{block_code}
<<lsdv_fe_str_disp, echo=FALSE, cache=TRUE, results='asis'>>=
  stargazer(reg_lsdv_ex,reg_fe_ex,type='latex',
    single.row=TRUE,omit.stat=c('all'),table.layout='-ldc-t-o-n',
    column.labels=c('LSDV','FE'),omit='Constant',
    omit.labels='Intercept?',omit.yes.no=c('Yes','Yes'))
@
\end{frame}

\begin{frame}[c]
  \begin{block}{FE and LSDV: differences}
    \begin{itemize}
      \item LSDV is more computationally demanding because you have so many more variables (individual dummies) included in LSDV
      \item Estimating LSDV with many individuals was computationally too demanding before
      \item But, the problem was solved (the $felm$ function from the $lfe$ package)
    \end{itemize}
  \end{block}
\end{frame}

%--- New Frame ---%

\begin{frame}
  \frametitle{Implementation of (LSDV) FE estimation in R}
  \begin{block}{$felm$ function from the $lfe$ package}
      \begin{align*}
        felm(y\sim x_1+\dots+x_k|fe\_vars|instrument|cluster\\
          ,data=data)
      \end{align*}
      \begin{itemize}
        \item $fe\_vars$: variable(s) that indicates which observations belongs in the same group
        \item $instrument$: more on this later
        \item $cluster$: variable that you desire to cluster the error around
      \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \frametitle{Implementation of LSDV (FE) estimation in R}
  \begin{block}{Model}
  \vspace{-0.6cm}
  \begin{align*}
    log(scrap_{i,t}) & = \beta_0 + \beta_1 grant_{i,t} + \beta_2 avgsal_{i,t} + \beta_3 tothrs_{i,t} \\
    & + \beta_4 union_{i,t} + \alpha_i + u_{i,t}
  \end{align*}
  \end{block}
  \begin{block_code}{R code: FE estimation}

<<lsdv_est_lfe, cache=TRUE,size='scriptsize',dependson='train', results='hide'>>=
  #=== FE estimation ===#
  reg_lsdv <- felm(log(scrap)~grant+avgsal+tothrs|fcode|0|0,
    data=data_tr)

  #=== summary of the estimation ===#
  summary(reg_lsdv)$coef
@
  \end{block_code}
\end{frame}


\begin{frame}[c,fragile]
  \frametitle{Equivalence of FE and LSDV}
<<lsdv_fe_compare, cache=FALSE,size='scriptsize',dependson='train',results='asis', echo=FALSE>>=
  stargazer(reg_fe,reg_lsdv,single.row=TRUE,omit.stat=c('all'),table.layout='-ldc-t-o-n',
  column.labels=c('FE','LSDV'),omit='Constant',type='latex')
@
\end{frame}



\begin{frame}[c,fragile]
  \begin{block}{Fixed Effects (FE) or First-difference (FD)}
    \begin{itemize}
      \item both transformations eliminate $\alpha_i$, which makes your estimation robust to the correlation between $\alpha_i$ and independent variables (\textcolor{blue}{This is the most important reason why we use FD and FE})
      \item FE is more/less efficient that FD depending on the characteristics of the error term (look at the Wooldridge textbook for more detail if you are interested)
      \item \textcolor{blue}{The standard practice is to use FE}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Unbalanced Panel}
    Panel data which do not record the observations for exactly the same periods for all the individuals
  \end{block}

  \begin{block_code}{R code: stargazer}
<<unbalanced, echo=FALSE, cache=TRUE>>=
  raw_data <- data.table(
    id = rep(1:2,each=3),
    year = rep(2015:2017,2),
    income = c(77,82,84,110,NA,131),
    educ = c(12:14,18,NA,20)
  )
  raw_data
@
  \end{block_code}
  \begin{block}{Consequence}
    As long as attrition of the observations (missing observations) is random, there is no consequence to your FE or FD estimation
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{Random Effects (RE) model}
    \begin{itemize}
      \item Can be more efficient than FE and POLS
      \item Correlation between $\alpha_i$ and independent variables are \textcolor{blue}{NOT} allowed, meaning RE estimators are biased due to the correlation
      \item Unless $\alpha_i$ and independent variables are not correlated (which does not hold most of the time unless you got data from controlled experiments), $RE$ is not an attractive option
      \item You almost never see this methods used in papers that use real world data (non-experimental data)
    \end{itemize}
  \end{block}
\end{frame}


\begin{frame}[c]
  \title{Hausman Test}
  \author{}
  \date{}
  \maketitle
\end{frame}

\begin{frame}[c]
  \begin{block}{Hausman test}
  \vspace{-0.6cm}
    \begin{align*}
    H_0:\;\; & \alpha_i \;\; \mbox{is not correlated with any indepdent variables}\\
    H_1:\;\; & \alpha_i \;\; \mbox{is correlated with at least one of indepdent variables}
    \end{align*}
  \end{block}

  \only<2->{\begin{block}{Idea}
  \begin{itemize}
    \item If $\alpha_i$ is not correlated with any independent variables, both RE and FE are unbiased and produce similar coefficient estimates
    \item If $\alpha_i$ is correlated with any independent variables, only FE is unbiased, and RE and FE should produce substantially different coefficient estimates
  \end{itemize}
  \end{block}}

  \only<3->{\begin{block}{Procedure}
  \begin{itemize}
    \item FE estimation
    \item RE estimation
    \item Compare the coefficient estimates between the two
  \end{itemize}
  \end{block}}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Model}
  \vspace{-0.6cm}
  \begin{align*}
    log(scrap_{i,t}) & = \beta_0 + \beta_1 grant_{i,t} + \beta_2 avgsal_{i,t} + \beta_3 tothrs_{i,t} \\
    & + \beta_4 union_{i,t} + \alpha_i + u_{i,t}
  \end{align*}
  \end{block}

  \begin{block_code}{R code: Hausman test}
<<hausman, cache=TRUE, size='scriptsize'>>=
  #--- FE ---#
  reg_fe <- plm(log(scrap)~grant+avgsal+tothrs,
    data=pdata_tr,model='within')

  #--- RE ---#
  # tell R that you want the model to be ''random''
  reg_re <- plm(log(scrap)~grant+avgsal+tothrs,
    data=pdata_tr,model='random')

  #--- Hausman test ---#
  # phtest() is from the plm package
  phtest(reg_fe, reg_re)
@
  \end{block_code}
\end{frame}


\begin{frame}[c,fragile]
  \begin{block}{Hausman test}
    \begin{itemize}
      \item If you reject the null, that means $\alpha_i$ is likely to be correlated with some independent variables $\Rightarrow$ RE estimation is biased
      \item If you do not reject the null, that means $\alpha_i$ is not likely to be correlated with any of the independent variables $\Rightarrow$ both FE and RE estimations are unbiased
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \title{Year Fixed Effects}
  \author{}
  \date{}
  \maketitle
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Year Fixed Effects (FE)}
    Just a collection of year dummies, which takes 1 if in a specific year, 0 otherwise.
  \end{block}
  \begin{block_code}{Example}
<<year_fe, echo=FALSE, cache=TRUE>>=
  year_fe <- data.table(
    id = rep(1:3,each=3),
    year = rep(2015:2017,3),
    income = c(77,82,84,110,120,131,56,60,61),
    educ = c(12:14,18:20,10:12),
    FE_2015 = rep(c(1,0,0),3),
    FE_2016 = rep(c(0,1,0),3),
    FE_2017 = rep(c(0,0,1),3)
  )
  year_fe
@
  \end{block_code}
\end{frame}

\begin{frame}[c]
  \only<1-2>{\begin{block}{What do year FEs do?}
    capture \textcolor{blue}{anything} that happened to all the individuals for a specific year relative to the base year
  \end{block}}
  \only<2-3>{\begin{block}{Example}
  Education and wage data from $2012$ to $2014$,
    \begin{align*}
      log(income) = & \beta_0 + \beta_1 educ + \beta_2 exper \\
     & + \sigma_1 FE_{2012} + \sigma_2 FE_{2013}
    \end{align*}
  \vspace{-0.6cm}
  \begin{itemize}
    \item $\sigma_1$: captures \textcolor{red}{whatever} the difference in $log(income)$ between $2012$ and $2014$
    \item $\sigma_2$: captures \textcolor{red}{whatever} the difference in $log(income)$ between $2013$ and $2014$
  \end{itemize}
  \end{block}}
  \only<3->{\begin{block}{Interpretation}
    $\sigma_1=0.05$ means that $log(income)$ is greater in $2012$ than $2014$ by $5\%$ on average for whatever reasons with everything else fixed.
  \end{block}}
\end{frame}

\begin{frame}[c]
  \only<1-2>{\begin{block}{Year FE when panel or IPCS datasets are used}
    It is almost always a good practice to include year FE.
  \end{block}}
  \only<2-3>{\begin{block}{Why?}
  \begin{itemize}
    \item Remember year FEs capture \textcolor{blue}{anything} that happened to all the individuals for a specific year relative to the base year
    \item In other words, \textcolor{blue}{all the unobserved factors} that are common to all the individuals in a specific year is \textcolor{blue}{controlled for (taken out of the error term)}
  \end{itemize}
  \end{block}}
  \only<3->{\begin{block}{Example}
    Economic trend in:
    \begin{align*}
      log(income) = & \beta_0 + \beta_1 educ + \beta_2 exper \\
     & + \sigma_1 FE_{2012} + \sigma_2 FE_{2013}
    \end{align*}
    \vspace{-0.6cm}
    \begin{itemize}
      \item Education is non-decreasing through time
      \item Economy might have either been going down or up during the observed period
    \end{itemize}
  \end{block}}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Other Examples}
    \begin{itemize}
      \item energy price (as long as everybody's facing the same price each year)
      \item crop price (as long as everybody's facing the same price each year)
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Caveats}
    \begin{itemize}
      \item year FEs would be perfectly collinear with a variable that changes only across time, but not across individuals
      \item if your variable of interest is such a variable, you cannot include year FEs, which would then make your estimation subject to omitted variable bias due to \textcolor{red}{other} unobserved yearly-changing factors
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \title{Inference using panel data}
  \author{}
  \date{}
  \maketitle
\end{frame}

\begin{frame}[c]
\begin{block}{Heteroskedasticity}
  Just like we saw for OLS using cross-sectional data, heteroskedasticity leads to biased estimation of the standard error of the coefficient estimators if not taken into account
\end{block}
\only<2->{\begin{block}{Serial correlation}
  Correlattion of errors over time, which we call \textcolor{blue}{serial correlation}
\end{block}}

\only<3->{\begin{block}{Consequences of serial correlation}
\begin{itemize}
  \item just like heteroskedasticity, serial correlation could lead to biased estimation of the standard error of the coefficient estimators if not taken into account
  \item do not affect the unbiasedness and consistency property of your estimators
\end{itemize}
\end{block}}
\end{frame}

\begin{frame}[c]
  \begin{block}{Important}
  \begin{itemize}
    \item Taking into account the potential of serial correlation when estimating the standard error of the coefficient estimators can dramatically change your conclusions about the statistical significance of some independent variables!!
    \item When serial correlation is ignored, you tend to underestimate the standard error (why?), inflating $t$-statistic, which in turn leads to over-rejection that you should.
  \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{Bertrand, Duflo, and Mullainathan (2004)}
  \begin{itemize}
    \item Examined how problematic serial correlation is in terms of inference via Monte Carlo simulation
    \begin{itemize}
      \item generate a fake treatment dummy variable in a way that it has no impact on the outcome (dependent variable) in the dataset of women's wages from the Current Population Survey (CPS)
      \item run regression of the oucome on the treatment variable
      \item test if the treatment variable has statistically significant effect via $t$-test
    \end{itemize}
    \item They rejected the null $67.5\%$ at the $5\%$ significance level!!
  \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
\begin{block}{When only heteroskedasticity is present}
  You can estimate the heteroskedasticity-consistent standard error using the $felm$ function
\end{block}

  \begin{block_code}{R code: heteroskedasticity-consistent se estimation when using panel data}
<<het_consis, cache=FALSE, dependson='FE_est'>>=
  #--- FE estimation ---#
  reg_fe <- felm(log(scrap)~grant+avgsal+tothrs|fcode+year|0|0,
    data=data_tr)

  summary(reg_fe)

  #=== robust se ===#
  se_robust <- summary(reg_fe,robust=TRUE)$coef[,2]

  #=== non-robust se ===#
  se_non_robust <- summary(reg_fe)$coef[,2]
@

  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]

  \begin{block_code}{R code: stargazer with robust se}
<<stg_het_consis, eval=FALSE, cache=TRUE,dependson='FE_est'>>=
  #--- stargazer ---#
  stargazer(reg_fe,reg_fe,se=list(se_robust,se_non_robust),
    column.labels=c('Het-robust','Non-robust'),type='latex')
@
  \end{block_code}

\footnotesize{
<<stg_het_consis_disp, echo=FALSE, cache=TRUE,dependson='FE_est',results='asis',dependson='het_consis'>>=
  #--- stargazer ---#
  stargazer(list(reg_fe,reg_fe),se=list(se_robust,se_non_robust),column.labels=c('Het-robust','Non-robust'), type='latex',single.row=TRUE,omit.stat=c('all'),table.layout='-ldc-t-')
@
}
\end{frame}

% \begin{frame}
% \begin{block}{When both heteroskedasticity and serial correlation are present}
% \begin{itemize}
%   \item There exists a heteroskedasticity and serial correlation-consistent standard error developed by Arellano (1987)
%   \item You can obtain Arellano estimates using $method='arellano'$ in the $vcovHC()$ function from the $plm$ package after you did panel data estimation
% \end{itemize}
% \end{block}
% \end{frame}

% \begin{frame}[c,fragile]
%   \begin{block_code}{R code: Arellano vcov estimation when using panel data}
%   <<A, cache=TRUE,dependson='FE_est'>>=
%   #--- FE estimation ---#
%   reg_fe <- plm(log(scrap)~grant+avgsal+tothrs,
%     data=pdata_tr,model='within',effect='twoway')

%   #--- Arellano vcov estimation ---#
%   vcov_Are <- vcovHC(reg_fe,method='arellano')

%   #--- recover the standard error ---#
%   se_Are <- sqrt(diag(vcov_Are))
%   @
%   \end{block_code}
% \end{frame}

% \begin{frame}[c,fragile]

%   \begin{block_code}{R code: stargazing two regressions results}
%   <<A_het, eval=FALSE, cache=TRUE,dependson='FE_est'>>=
%   #--- stargazer ---#
%   stargazer(reg_fe,reg_fe,se=list(se_het,se_Are),type='latex',
%     table.layout='-ldc-t-',column.labels=c('Het-robust','Arellano'),
%     no.space=TRUE)
%   @
%   \end{block_code}

%   <<nw_het_disp, echo=FALSE, cache=TRUE,dependson='FE_est',results='asis'>>=
%   #--- stargazer ---#
%   stargazer(reg_fe,reg_fe,se=list(se_het,se_Are),type='latex',table.layout='-ldc-t-',column.labels=c('Het-robust','Arellano'),no.space=TRUE)
%   @

% \end{frame}

\begin{frame}[c]
  \begin{block}{When both heteroskedasticity and serial correlation are present}
  \begin{itemize}
    \item You can take into account \textcolor{blue}{both} heteroskedasticity and serial correlation by clustering by individual (whatever the unit of individual is: state, county, farmer)
    \item cluster by individual allows correlation within individuals (over time)
  \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: Cluster by individual}
<<cluster, cache=TRUE,dependson='FE_est'>>=
  #--- FE estimation ---#
  reg_fe <- felm(log(scrap)~grant+avgsal+tothrs|fcode+year|0|fcode,
    data=data_tr)

  #=== extract se ===#
  se_cluster <- summary(reg_fe)$coef[,2]
@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]

  \begin{block_code}{R code: stargazing three regressions results}
<<cluster_het, eval=FALSE, cache=TRUE,dependson='FE_est'>>=
  #--- stargazer ---#
  stargazer(list(reg_fe,reg_fe),se=list(se_robust,se_cluster),
    type='latex',table.layout='-ldc-t-',
    column.labels=c('Het-robust','Cluster (indiv)'),no.space=TRUE)
@
  \end{block_code}

\footnotesize{
<<cluster_het_disp, echo=FALSE, cache=FALSE, dependson='FE_est',results='asis'>>=
  #--- stargazer ---#
  stargazer(reg_fe,reg_fe,se=list(se_robust,se_cluster),type='latex',
    column.labels=c('Het-robust','Cluster (indiv)'),
    no.space=TRUE,omit.stat=c('all'),table.layout='-ldc-t-')
@
}

\end{frame}

\end{document}

