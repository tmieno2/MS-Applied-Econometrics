---
title: "Discrete Choice"
author: "AECN 396/896-002"
output:
  xaringan::moon_reader:
    # css: [default, metropolis, metropolis-fonts] 
    css: xaringan-themer.css 
    self_contained: true
    nature:
      ratio: 4:3
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: middle

```{r, child = './../setup.Rmd'}
```

```{r echo = F, eval = F}
setwd(here("LectureNotes/DiscreteChoice"))
```

```{r additional-libraries, include = F, cache = F}
#--- load packages ---#
library(broom)
library(margins)
library(here)
library(fixest)
library(lmtest)
library(modelsummary)
library(stargazer)
```

# Discrete Choice Analysis
+ Focus on understanding choices that are discrete (not continuous)
  * Whether you own a car or not (binary choice)
  * Whether you use an iPhone, Android, or other types of cell phones (Multinomial choice)
  * Which recreation sites you visit this winter (multinomial)
+ Linear models we have seen are often not appropriate

---
class: middle

# Binary Response Model  

.content-box-green[**Binary response**]

$y = 0 \;\; \mbox{(if you do not own a car)}$ 

$y = 1 \;\; \mbox{(if you own at least one car)}$

<br>

.content-box-green[**Question we would like to answer**]

How do variables $x_1,\dots,x_k$ affect the status of $y$ (the choice of whether to own at least one car or not)?

---
class: middle

.content-box-green[**Binary response**]

We try to model the .red[probability] of $y = 1$ (own at least one car)

$Pr(y=1|x_1,\dots,x_k) = f(x_1,\dots,x_k)$

as a function of independent variables.

---
class: middle

.content-box-green[**Linear Probability Model**]

$Pr(y=1|x_1,\dots,x_k) = \beta_0+\beta_1 x_1+\dots+\beta_k x_k$

--

<br>

.content-box-green[**Drawback**]

There is no guarantee that the predicted probability is bounded within [0, 1].

---
class: middle


.content-box-green[**How about this?**]

$Pr(y=1|x_1,\dots,x_k) = G(\beta_0+\beta_1 x_1+\dots+\beta_k x_k)$

where $0<G(z)<1$ for all real numbers $z$

---
class: middle

Different choices of $G()$ lead to different models.


.content-box-green[**Logit model**]

$G(z) = exp(z)/[1+exp(z)] = \frac{e^z}{1+e^z}$

where $z = \beta_0+\beta_1 x_1+\dots+\beta_k x_k$

<br>

.content-box-green[**Probit model**]

$G(z) = \Phi(z)$

where $\Phi(z)$ is the standard normal cumulative distribution function

---
class: middle

This what $G()$ looks like for logit and probit.

```{r echo = F, out.width = "70%"}
plot_data <- data.table(z = seq(-3, 3, length = 1000)) %>%
  mutate(logit = exp(z) / (1 + exp(z)), normal = pnorm(z)) %>%
  melt(id.vars = "z") %>%
  data.table()

g_G <- ggplot(data = plot_data) +
  geom_line(aes(x = z, y = value, color = variable), size = 1) +
  ylab("y") +
  scale_color_discrete(name = "") +
  theme(
    legend.position = "bottom"
  )
g_G
```

---
class: middle


$Pr(y=1|x_1,\dots,x_k) = G(\beta_0+\beta_1 x_1+\dots+\beta_k x_k)$

+ What do $\beta$s measure?
+ How do we interpret them?

---
class: middle

$\mbox{Before:} x_1=0,\dots,x_k=0 \Rightarrow z=\beta_0$ 

$\mbox{After:}\;\; x_1=1 \;\;\mbox{and}\;\; x_2=0,\dots,x_k=0 \Rightarrow z=\beta_0+\beta_1$


```{r echo = F}

g_gz <- ggplot(data = plot_data[variable == "normal", ]) +
  geom_line(aes(x = z, y = value), size = 1) +
  geom_hline(yintercept = 0, size = 0.2) +
  #--- before ---#
  geom_point(aes(y = 0, x = -1), color = "red") +
  geom_text(
    aes(y = -0.06, x = -1, label = "before"),
    color = "blue",
    size = 3
  ) +
  annotate(
    "text",
    y = -0.14, x = -1, label = expression(beta[0]),
    color = "blue", size = 3
  ) +
  geom_point(aes(y = pnorm(-1), x = -1), color = "red") +
  geom_segment(
    aes(y = pnorm(-1), x = -1, yend = 0, xend = -1),
    color = "red",
    linetype = 2
  ) +
  #--- after ---#
  geom_point(aes(y = 0, x = 1), color = "red") +
  geom_text(
    aes(y = -0.06, x = 1, label = "after"),
    color = "blue", size = 3
  ) +
  annotate(
    "text",
    y = -0.14, x = 1, label = expression(beta[0] + beta[1]),
    color = "blue", size = 3
  ) +
  geom_point(aes(y = pnorm(1), x = 1), color = "red") +
  geom_segment(aes(y = pnorm(1), x = 1, yend = 0, xend = 1), color = "red", linetype = 2) +
  #--- change in z ---#
  geom_segment(
    aes(x = -1, y = 0, xend = 1, yend = 0),
    arrow = ggplot2::arrow(length = unit(0.05, "npc")),
    color = "red", size = 0.5
  ) +
  ylab("G(z)") +
  scale_color_discrete(name = "") +
  theme(
    legend.position = "bottom"
  )
```

```{r echo = F, out.width = "70%"}
g_gz
```

---
class: middle

```{r echo = F, out.width = "60%"}
g_gz
```

+ $\beta$s measure how far you move along the x-axis

+ $\beta$s does not directly measure how independent variables influence the probability of $y=1$

---
class: middle

To understand the marginal impact of $x_k$ on $Prob(y=1)$ (how a change in $x_k$ affects the likelihood of owning a car), you need to do a bit of math.

<br>

.content-box-green[**Model**]

$Pr(y=1|x_1,\dots,x_k) = G(z)$ 

$z = \beta_0+\beta_1 x_1+\dots+\beta_k x_k$

<br>

.content-box-green[**marginal impact**]

Differentiating both sides with respect to $x_k$,

$\frac{\partial Pr(y=1|x_1,\dots,x_k)}{\partial x_k} = G'(z)\times \frac{\partial z}{\partial x_k}$ 

$\qquad\qquad\qquad\:\: = G'(z)\times \beta_k$

---
class: middle

.content-box-green[**marginal impact**]

Differentiating both sides with respect to $x_k$,

$\frac{\partial Pr(y=1|x_1,\dots,x_k)}{\partial x_k} = G'(z)\times \frac{\partial z}{\partial x_k}$ 

$\qquad\qquad\qquad\:\: = G'(z)\times \beta_k$

<br>

.content-box-green[**Notes**]

+ The marginal impact of an independent variable depends on the values of all the independent variables: $G(\beta_0+\beta_1 x_1+\dots+\beta_k x_k)$

+ Since $G'()$ is always positive, the sign of the marginal impact of an independent variable on $Prob(y=1)$ is always the same as the sign of its coefficient

---
class: middle

.content-box-green[**Estimation of Binary Choice Models**]

+ Linear models: OLS

+ Binary choice models: .blue[Maximum Likelihood Estimation (MLE)]

---
class: middle


.content-box-green[**OLS**]

Find parameters that makes the sum of residuals squared the smallest

<br>

.content-box-green[**MLE (very loosely put)**]

Find parameters $(\beta$s) that makes what we observed (collection of binary decisions made by different individuals) most likely (.blue[Maximum Likelihood])

---
class: middle


.content-box-green[**Observed decisions made by two individuals**]

+ Individual 1: $y=1$ (own at least one car)

+ Individual 2: $y=0$ (does not own a car)

--

<br>

.content-box-green[**Probability of individual decisions**]

+ $\mbox{Individual 1}: Prob(y_1=1|\mathbf{x_1})= G(z_1)$

+ $\mbox{Individual 2}: Prob(y_2=0|\mathbf{x_2})= 1-G(z_2)$

where 

+ $\mathbf{x_i}$ is a collection of independent variables for individual $i$ $(x_{1,i}, \dots, x_{k,i})$.

+ $z_i = \beta_0+\beta_1 x_{1,i}+\dots+\beta_k x_{k,i}$ 

--

<br>

.content-box-green[**Probability of a collection of decisions**]

The probability that we observe a .blue[collection of choices] made by them (if their decisions are independent)

$Prob(y_1=1|\mathbf{x_1})\times Prob(y_2=0|\mathbf{x_2}) = G(z_1)\times [1-G(z_2)]$
  
which we call .blue[likelihood function].

---
class: middle

.content-box-green[**Probability of a collection of decisions**]

The probability that we observe a .blue[collection of choices] made by them (if their decisions are independent)

$Prob(y_1=1|\mathbf{x_1})\times Prob(y_2=0|\mathbf{x_2}) = G(z_1)\times [1-G(z_2)]$
  
which we call .blue[likelihood function].

--

<br>

.content-box-green[**MLE**]

$Max_{\beta_1,\dots,\beta_k}\;\; G(z_1)\times [1-G(z_2)]$

---
class: middle

.content-box-green[**MLE of Binary Choice Model in General**]

Maximize the likelihood function:

$Max_{\beta_1,\dots,\beta_k}\;\; L$

where $L=\Pi_{i=1}^n \Big[y_i\times G(z_i)+(1-y_i)\times(1-G(z_i))\Big]$ is the likelihood function.

<br>

.content-box-green[**Log-likelihood function**]
    
$LL = log\Big(\Pi_{i=1}^n \Big[y_i\times G(z_i)+(1-y_i)\times(1-G(z_i))\Big]\Big)$ 
$\qquad = \sum_{i=1}^n log\Big(y_i\times G(z_i)+(1-y_i)\times(1-G(z_i))\Big)$

<br>

.content-box-green[**MLE with $LL$**]

$argmax_{\beta_1,\dots,\beta_k}\;\; L \equiv argmax_{\beta_1,\dots,\beta_k}\;\; LL$

---
class: middle

# Implementation in R with an example

Participation of females in labor force:

$Pr(inlf=1|\mathbf{x})= G(z)$

where

$z = \beta_0+\beta_1 nwifeinc+ \beta_2 educ+ \beta_3 exper$ 

$\quad\;\; + \beta_4 exper^2 + \beta_5 age + \beta_6 kidslt6 + \beta_7 kidsge6$

+ $inlf$: 1 if in labor force in 1975, 0 otherwise
+ $nwifeinc$: earning as a family if she does not work
+ $kidslt6$: \# of kids less than 6 years old
+ $kidsge6$: \# of kids who are 6-18 year old

---
class: middle

```{r }
#--- import the data ---#
data <- read.dta13("MROZ.dta") %>%
  mutate(exper2 = exper^2)

#--- take a look ---#
dplyr::select(data, inlf, nwifeinc, kidslt6, kidsge6, educ) %>%
  head()
```

---
class: middle

```{r echo = F}
dplyr::select(data, inlf, nwifeinc, kidslt6, kidsge6, educ) %>%
  head()
```

For individual 1 (row 1 of the data),

$z_1 = \beta_0+\beta_1 `r round(data[1, ]$nwifeinc, digits = 2)`+ \beta_2 `r round(data[1, ]$educ)`+ \beta_3 `r round(data[1, ]$exper)` + \beta_4 `r round(data[1, ]$exper2)` + \beta_5 `r round(data[1, ]$age)` + \beta_6 `r round(data[1, ]$kidslt6)` + \beta_7 `r round(data[1, ]$kidsge6)`$

The probability that individual 1 would make the decision he/she made given $\beta$s is:

$G(z_1)$ (a function of $\beta$s)

---
class: middle

```{r echo = F}
dplyr::select(data, inlf, nwifeinc, kidslt6, kidsge6, educ) %>%
  tail()
```

For individual 753 (row 753 of the data),

$z_{753} = \beta_0+\beta_1 `r round(data[753, ]$nwifeinc, digits = 2)`+ \beta_2 `r round(data[753, ]$educ)`+ \beta_3 `r round(data[753, ]$exper)` + \beta_4 `r round(data[753, ]$exper2)` + \beta_5 `r round(data[753, ]$age)` + \beta_6 `r round(data[753, ]$kidslt6)` + \beta_7 `r round(data[753, ]$kidsge6)`$

The probability that individual 753 would make the decision he/she made given $\beta$s is:

$1 - G(z_{753})$ (a function of $\beta$s)

---
class: middle

Multiply all the probabilities of observed choices given $\beta$s,

$L = G(z_1) \times G(z_2) \times \dots [1-G(z_753)]$

--

$LL = log\Big(G(z_1) \times G(z_2) \times \dots [1-G(z_753)]\Big)$

--

Solve the following problems to estimate $\beta$s:

$Max_{\beta_1, \dots, \beta_7} \quad LL$

---
class: middle


.content-box-green[**Estimating binary choice model using $R$**]

You can use the `glm()` function (no new packages installation necessary) when using cross-sectional data

+ `glm` refers to Generalized Linear Model, which encompass linear models we have been using

+ you specify the `family` option to tell what kind of model you are estimating

---
class: middle


.content-box-green[**Probit model estimation**]

```{r }
probit_lf <- glm(
  #--- formula ---#
  inlf ~ nwifeinc + educ + exper + exper2 + age + kidslt6 + kidsge6,
  #--- data ---#
  data = data,
  #--- models ---#
  family = binomial(link = "probit")
)
```

<br>

.content-box-green[**family option**]

+ `binomial()`: tells R that your dependent variable is binary

+ `link = "probit"`: tells R that you want to use the cumulative distribution function of the standard normal distribution as $G()$ in $Prob(y=1|\mathbf{x})=G(z)$

---
class: middle

.left5[
```{r model-sum-glm-1, eval = F}  
msummary(
  probit_lf,
  stars = TRUE,
  gof_omit = "IC|F",
  output = "flextable"
) %>%
  fontsize(
    size = 9,
    part = "all"
  ) %>%
  autofit()
```
]

.right5[
```{r model-sum-glm-1-f, ref.label = "model-sum-glm-1", echo = F}  

```
]

---
class: middle


.content-box-green[**Logit model estimation**]

```{r }
logit_lf <- glm(
  #--- formula ---#
  inlf ~ nwifeinc + educ + exper + exper2 + age + kidslt6 + kidsge6,
  #--- data ---#
  data = data,
  #--- models ---#
  family = binomial(link = "logit")
)
```

.content-box-green[**family option**]

+ `binomial()`: tells R that your dependent variable is binary

+ `link = "logit"`: tells $R$ that you want to use $G(z) = \frac{e^z}{1+e^z}$ in $Prob(y=1|\mathbf{x})=G(z)$

---
class: middle

.left5[
```{r model-sum-glm, eval = F}  
msummary(
  logit_lf,
  stars = TRUE,
  gof_omit = "IC|F",
  output = "flextable"
) %>%
  fontsize(
    size = 9,
    part = "all"
  ) %>%
  autofit()
```
]

.right5[
```{r model-sum-glm-f, ref.label = "model-sum-glm", echo = F}  

```
]

---
class: middle

.content-box-green[**Important**]

+ You <span style = "color: red;"> cannot </span> directly compare the coefficient on the same variable from probit and logit! The fact that the coefficient on `educ` is higher from the logit model does not mean the logit model is suggesting `educ` is more influential than the probit model suggests. They are on different scales. 


---
class: middle

# Post-estimation operations and diagnostics

---
class: middle

.content-box-green[**Log-likelihood (fitted)**]

$LL =\sum_{i=1}^n log\Big(y_i\times G(\hat{z}_i)+(1-y_i)\times(1-G(\hat{z}_i))\Big)$

+ $\hat{z}_i=\hat{\beta}_0+\hat{\beta}_1 x_1+\dots+\hat{\beta}_k x_k$

+ $G(\hat{z}_i)$ is the fitted value of $Prob(y=1|\mathbf{x})$

<br>

.content-box-green[**Example**]

+ $G(\hat{z}_i) = 0.9$: predicted that individual $i$ is very likely to own a car 
+ $y_i = 0$: in reality, individual $i$ does not own a car

$\Rightarrow$

$log\Big(0\times 0.9 +(1-0)\times(1-0.9)\Big) = log(0.1) = `r round(log(0.1), digits = 2)`$

---
class: middle

.content-box-green[**Log-likelihood (fitted)**]

$LL =\sum_{i=1}^n log\Big(y_i\times G(\hat{z}_i)+(1-y_i)\times(1-G(\hat{z}_i))\Big)$

+ $\hat{z}_i=\hat{\beta}_0+\hat{\beta}_1 x_1+\dots+\hat{\beta}_k x_k$

+ $G(\hat{z}_i)$ is the fitted value of $Prob(y=1|\mathbf{x})$

<br>

.content-box-green[**Example**]

+ $G(\hat{z}_i) = 0.9$: predicted that individual $i$ is very likely to own a car 
+ $y_i = 1$: in reality, individual $i$ indeed owns a car

$\Rightarrow$

$log\Big(1\times 0.9 +(1-1)\times(1-0.9)\Big) = log(0.9) = `r round(log(0.9), digits = 2)`$

---
class: middle

.content-box-green[**Log-likelihood (fitted)**]

So, the better your prediction (model fit) is, the .blue[the greater (less negative) LL is.]

---
class: middle

.content-box-green[**McFadden's**] pseudo- $R^2$

A measure of how much better your model is compared to the model with only the intercept.

$pseudo-R^2=1-LL/LL_0$

where $LL_0$ is the log-likelihood when you include only the intercept.

---
class: middle


.content-box-green[**R code**]

```{r }
logit_lf_0 <- glm(
  inlf ~ 1,
  data = data,
  family = binomial(link = "logit")
)

#--- extract LL using the logLik() function ---#
(LL0 <- logLik(logit_lf_0))

#--- extract LL using the logLik() function from your preferred model ---#
(LL <- logLik(logit_lf))

#--- pseudo R2 ---#
1 - LL / LL0
```
---
class: middle


.content-box-green[**Alternatively**]

```{r }
#--- or more easily ---#
1 - logit_lf$deviance / logit_lf$null.deviance

#--- what are deviances? ---#
logit_lf$null.deviance # = -2*LL0
logit_lf$deviance # = -2*LL
```

+ `null.deviance` $= -2\times LL_0$
+ `deviance` $= -2\times LL$

---
class: middle

.content-box-green[**Testing joint significance**]

You can do Likelihood Ratio (LR) test:

$LR = 2(LL_{unrestricted}-LL_{restricted}) \sim \chi^2_{df\_restrictions}$

where $df\_restrictions$ is the number of restrictions.

<br>

.content-box-green[**Note**]

LR test is very similar conceptually to F-test.

<br>

.content-box-green[**Example**]

+ $H_0:$ the coefficients on $exper$, $exper2$, and $age$ are $0$

+ $H_1:$ $H_0$ is false

---
class: middle

```{r }
#--- unrestricted ---#
logit_ur <- glm(
  inlf ~ nwifeinc + educ + exper + exper2 + age + kidslt6 + kidsge6,
  data = data, family = binomial(link = "logit")
)

#--- restricted ---#
logit_r <- glm(
  inlf ~ nwifeinc + educ + kidslt6 + kidsge6,
  data = data, family = binomial(link = "logit")
)

#--- LR test using lrtest() from the lmtest package ---#
library(lmtest)
lrtest(logit_r, logit_ur)
```
---
class: middle

# Prediction

After estimating a binary choice model, you can easily predict the following two

+ $\hat{z}=\hat{\beta}_0+\hat{\beta}_1 x_1+\dots+\hat{\beta}_k x_k$

+ $\widehat{Prob(y=1|\mathbf{x})}=G(\hat{z})=G(\hat{\beta}_0+\hat{\beta}_1 x_1+\dots+\hat{\beta}_k x_k)$

---
class: middle

.content-box-green[**R code**]

```{r }
#--- z hat ---#
z <- predict(probit_lf, type = "link")
head(z)

#--- G(z) hat ---#
Gz <- predict(probit_lf, type = "response")
head(Gz)
```

---
class: middle

# Marginal effect of an independent variable

+ Coefficient estimates across different models (probit and logit) are not meaningful because the same value of a coefficient estimate means different things

+ They are the estimates of $\beta$s, not the direct impact of the independent variables on the $Prob(y=1)$

---
class: middle

Marginal effect of an independent variable

$\frac{\partial Pr(y=1|x_1,\dots,x_k)}{\partial x_k} = G'(z)\times \beta_k$

+ the marginal impact depends on the current levels of all the independent variables
+ we typically report one of the two types of marginal impacts
  * (becoming obsolete) the marginal impact .blue[at the mean] (average person): when all the independent variables take on their respective means
  * the average of the marginal impacts calculated for each of all the individuals observed

---
class: middle

.content-box-green[**Marginal impact at the mean**]

$\frac{\partial Pr(y=1|x_1,\dots,\bar{x_k})}{\partial x_k} = G'(\beta_0+\beta_1 \bar{x_1}+\dots+\beta_k\bar{x_k})\times \beta_k$

<br>

.content-box-green[**Mean marginal impact (MME)**]

$\sum_{i=1}^n \frac{\partial Pr(y_i=1|x_{i,1},\dots,x_{i,k})}{\partial x_k} = \sum_{i=1}^n G'(z_i)\times \beta_k$

---
class: middle


.content-box-green[**R codes to get MME**]

$$
\begin{aligned}
\hat{z}=\hat{\beta}_0+\hat{\beta}_1 x_1+\dots+\hat{\beta}_k x_k
\end{aligned}
$$ 

```{r }
#--- get z for all the individuals ---#
z <- predict(probit_lf, type = "link")
```

--

$$
\begin{aligned}
G'(\beta_0+\beta_1 \bar{x_1}+\dots+\beta_k\bar{x_k})
\end{aligned}
$$
where $G(z)$ is the cumulative distribution function for the standard normal distribution.

```{r }
#--- get G'(z) ---#
Gz_indiv <- dnorm(z)
```

--

$$
\begin{aligned}
G'(\beta_0+\beta_1 \bar{x_1}+\dots+\beta_k\bar{x_k})
\end{aligned}\times \beta_k
$$

```{r }
#--- mean marignal impact of eduction ---#
mean(Gz_indiv) * probit_lf$coef["educ"]
```

---
class: middle

Fortunately, the `margins` package provides you with a more convenient way of calculating MMEs.

```{r }
library(margins)

#--- calculate MME based on the probit estimation ---#
mme_lf <- margins(probit_lf, type = "response")

#--- get the summary ---#
summary(mme_lf)
```

For example, the average marginal effect of `educ` on the probability of "in labor force" is `r data.table(summary(mme_lf))[factor == "educ", AME] %>% round(digits = 3)`. That is if education increases by one year, then the probability of being in labor force increases by 4%.

---
class: inverse, center, middle

# Multinomial Choice Model

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---
class: middle

# Multinomial Choice 

Instead of two options, you are picking one option out of more than two options

+ which carrier?
  * Verizon
  * Sprint
  * AT\&T
  * T-mobile
+ which transportation means to commute?
  * drive
  * Uber
  * bus
  * train
  * bike

---
class: middle

# Multinomial logit model

The most popular model to analyze multinomial choice

+ environmental evaluation

+ tranposrtation

+ marketing

---
class: middle

# Understanding multinomial logit model


.content-box-green[**Choice of train route options**]

+ 10 euros, 30 minutes travel time, one change

+ 20 euros, 20 minutes travel time, one change

+ 22 euros, 22 minutes travel time, no change

--

<br>

.content-box-green[**Associated utility**]

+ $V_1=\alpha_1 + \beta 10 + \gamma 30 + \rho 1 + v_1$

+ $V_2=\alpha_2 + \beta 20 + \gamma 20 + \rho 1 + v_2$

+ $V_3=\alpha_3 + \beta 22 + \gamma 22 + \rho 0 + v_3$

---
class: middle

.content-box-green[**Choice probability**]

Logit model assumes that the probability of choosing an alternative is the following:

+ $P_1=\frac{e^{V_1}}{e^{V_1}+e^{V_2}+e^{V_3}}$

+ $P_2=\frac{e^{V_2}}{e^{V_1}+e^{V_2}+e^{V_3}}$

+ $P_3=\frac{e^{V_3}}{e^{V_1}+e^{V_2}+e^{V_3}}$

--

.content-box-green[**Notes**]

+ $0<P_j<1$, $^\forall j=1,2,3$

+ $\sum_{j=1}^3=1$

---
class: middle

.content-box-green[**Modeled probability of choices**]

Modeled probability of observing individual $i$ choosing the option $i$ chose

$P_i = \Pi_{j=1}^3 y_{i,j}\times P_j$

where $y_{i,j}=1$ if $i$ chose $j$, 0 otherwise.

<br>

.content-box-green[**Example**]

$y_{i,1} = 0, \;\;y_{i,2} = 1,\;\;y_{i,3} = 0$

$P_i = \Pi_{j=1}^3 y_{i,j}\times P_j = 0\times P_1 + 1\times P_2 + 0\times P_3$

---
class: middle

The probability of observing a series of chocies made by all the subjects is
  
$LL = \Pi_{i=1}^n P_i = \Pi_{i=1}^n \Pi_{j=1}^3 y_{i,j}\times P_j$

if choices made by the subjects are independent with each other.

--

<br>

.content-box-green[**MLE**]

$Max_{\beta,\gamma,\rho}\;\; log(LL)$

---
class: middle

# Interpretation of the coefficients

.content-box-green[**Model in general**]

$V_{i,j} = \alpha_j + \beta_1 x_{1,i,j} + \dots + \beta_k x_{k,i,j}$ 

$P_{i,j} = \frac{e^{V_{i,j}}}{\sum_{k=1}^J e^{V_{i,k}}}$

--

<br>

.content-box-green[**Interpretation of the coefficients**]
  
$\frac{\partial P_{i,j}}{\partial x_{k,i,j}} = \beta_k P_{i,j}(1-P_{i,j})$
    
+ A marginal change in $k$th variable for alternative $j$ would change the probability of choosing alternative $j$ by $\beta_k P_{i,j}(1-P_{i,j})$
    
+ the sign of the impact is the same as the sign of the coefficient

---
class: middle

.content-box-green[**Implementation in $R$**]

You can use $mlogit$ package to estimate multinomial logit models:

+ format your data in a specific manner

+ convert your data using $mlogit.data()$

+ estimate the model using $mlogit()$

---
class: middle

```{r }
#--- library ---#
library(mlogit)

#--- get the travel mode data from the mlogit package ---#
data("TravelMode", package = "AER")

#--- take a look at the data ---#
# first 10 rows
head(TravelMode, 10)
```


---
class: middle

.content-box-green[**R code: data preparation**]

```{r mlogit_convert}
#--- convert the data ---#
TM <- mlogit.data(TravelMode,
  shape = "long", # what format is the data in?
  choice = "choice", # name of the variable that indicates choice made
  chid.var = "individual", # name of the variable that indicates who made choices
  alt.var = "mode" # the name of the variable that indicates options
)
```

---
class: middle

```{r eval = F}
#--- take a look at the data ---#
# first 10 rows
head(TM, 10)
```

.scroll-box-16[
```{r echo = F}
head(TM, 10)
```
]

---
class: middle


```{r }
#--- estimate ---#
ml_reg <- mlogit(choice ~ wait + vcost + travel, data = TM)
```

---
class: middle

.scroll-box-24[
```{r }
summary(ml_reg)
```
]

---
class: middle
  
# Understanding the results

```{r }
summary(ml_reg)$coef
```

+ intercept for $air$ is dropped $(air$ is the base)
  * train:(intercept) is $-0.786$ means that train is less likely to be chosen if all the other .blue[included] variables are the same

+ the greater the travel time, the less likely the option is chosen

---
class: inverse, center, middle

# Count data (Poisson)

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---
class: middle

# Count data (Poisson)

Count variables take <span style = "color: blue;"> non-negative discrete integers </span>  $(0,1,\dots,)$

+ the number of times individuals get arrested in a year
+ the number of cars owned by a family
+ the number of kids in a family

---
class: middle

# Poisson regression

By far the most popular choice to analyze count variables is <span style = "color: blue;"> Poisson regression </span>

+ The outcome (count) variable is assumed to be Poisson distributed
+ The mean of the Poisson distribution is assumed to be a function of some variables you believe matter

--
<br>

.content-box-green[**Poisson distribution**]

Poisson distribution is a discrete probability distribution that describes the probability of the number of events that occur in a fixed interval of time and/or space 

$$
\begin{aligned}
Prob(y|\lambda)=\frac{\lambda^y e^{-\lambda}}{y!}, \;\; \mbox{where} \;\; \lambda=E[y]
\end{aligned}
$$

---
class: middle
```{r poisson, echo=FALSE}
x <- 0:15
plot_data <- data.table(
  x = x,
  " E[y]=1 " = dpois(x, lambda = 1),
  " E[y]=3 " = dpois(x, lambda = 3),
  " E[y]=5 " = dpois(x, lambda = 5)
) %>%
  melt(id.vars = "x")

ggplot(data = plot_data) +
  geom_bar(
    aes(x = factor(x), y = value, fill = variable),
    color = "black", stat = "identity", position = "dodge"
  ) +
  ylab("probability") +
  scale_fill_discrete(name = "") +
  xlab("the number of events") +
  theme(
    legend.position = "bottom"
  )
```

.content-box-green[**Poisson regression**]

We try to learn what and how variables affect the <span style = "color: blue;"> expected value </span> (the expected number of events conditional on independent variables).

---
class: middle

.content-box-green[**Expected number of events conditional on independent variables**]

$$
\begin{aligned}
E[y|\mathbf{x}] = G(\beta_0+\beta_1 x_1+\dots + \beta_k x_k)
\end{aligned}
$$

+ This is exactly the same modeling framework we used
  - Linear model: $G(z)=z$
  - Probit model: $G(z)=\Phi(z)$

--
<br>

.content-box-green[**A popular choice of **]$G()$
$$
\begin{aligned}
  G(z) = exp(z)
\end{aligned}
$$

This ensures that the expected value conditional on $\mathbf{x}$ is always positive

---
class: middle

.content-box-green[**The number of events for two individuals**]

+ Individual 1: $y=3$ (own three cars)
+ Individual 2: $y=1$ (own one car)

--
<br>

.content-box-green[**Expected number of events observed**]
+ Individual 1: $\lambda_1= exp(z_1)$
+ Individual 2: $\lambda_2= exp(z_2)$

---
<br>
.content-box-green[**Probability of observing the number of events we observed**]

$$
\begin{aligned}
\mbox{Individual 1}:& Prob(y=3|\mathbf{x_1})=\frac{\lambda_1^3 e^{-\lambda_1}}{3!}  \\
\mbox{Individual 2}:& Prob(y=1|\mathbf{x_2})= \frac{\lambda_2^1 e^{-\lambda_2}}{1!}
\end{aligned}
$$

--
<br>    

.content-box-green[**Probability of observing a series of events by all individuals**]

The probability that we observe the collection of choices made by them (if their events are independent)

$$
\begin{aligned}
L=Prob(y_1=3|\mathbf{x_1})\times Prob(y_2=1|\mathbf{x_2}) = \frac{\lambda_1^3 e^{-\lambda_1}}{3!}\times\frac{\lambda_2^1 e^{-\lambda_2}}{1!}
\end{aligned}
$$

which we call llikelihood function.

--
<br>

.content-box-green[**Log-likelihood function**]

$$
\begin{aligned}
LL=log(L)&= log(\frac{\lambda_1^3 e^{-\lambda_1}}{3!}) + log(\frac{\lambda_2^1 e^{-\lambda_2}}{1!})
\end{aligned}
$$

(Remember $\lambda_i=exp(\beta_0+\beta_1 x_{i,1}+\dots + \beta_k x_{i,k})$)

--
<br>

$$
\begin{aligned}
Max_{\beta_1,\dots,\beta_k}\;\; & LL
\end{aligned}
$$

---
class: middle

# Implementation in R with an example

The number of times a man is arrested during 1986:
$$
\begin{aligned}
Pr(narr86|\mathbf{x})= G(z)
\end{aligned}
$$

where

$$
\begin{aligned}
z = & \beta_0+\beta_1 pcnv+ \beta_2 tottime + \beta_3 qemp86 + \beta_4 inc86\\
& + \beta_5 black + \beta_6 hispan
\end{aligned}
$$

+ $narr86$: \# of times arrested in 1986
+ $pcnv$: proportion of prior conviction
+ $tottime$: time in prison since 18
+ $qemp86$: \# quarters employed in 1986
+ $inc86$: legal income in 1986 (in $\$100$)


---
class: middle

R code: importing the data

```{r data_pois, cache=TRUE}
#--- import the data ---#
data <- read.dta13("CRIME1.dta")

#--- take a look ---#
dplyr::select(data, narr86, pcnv, qemp86, inc86) %>%
  head()
```

---
class: middle

.content-box-green[**R code: Poisson model estimation using glm()**]

```{r pois, cache=TRUE}
pois_lf <- glm(

  #--- formula ---#
  narr86 ~ pcnv + tottime + qemp86 + inc86 + black + hispan,

  #--- data ---#
  data = data,

  #--- models ---#
  family = poisson(link = "log")
)
```

.content-box-green[**`family` option**]

+ `poisson()`: tells $R$ that your dependent variable is Poisson distributed
+ `link = "log"`: tells $R$ that you want to use $exp()$ (the inverse of $log()$) as $G()$ in $E(y=1|\mathbf{x})=G(z)$

---
class: middle

.left4[
```{r, eval = F, cache=TRUE}
msummary(
  pois_lf,
  # keep these options as they are
  stars = TRUE,
  gof_omit = "IC|Log|Adj|F|Pseudo|Within"
)
```
]

.right6[
```{r, cache = TRUE, echo = F}
msummary(
  pois_lf,
  # keep these options as they are
  stars = TRUE,
  gof_omit = "IC|Log|Adj|F|Pseudo|Within",
  output = "flextable"
) %>%
  fontsize(
    size = 9,
    part = "all"
  ) %>%
  autofit()
```
]

---
class: middle

# Calculate average marginal effects

Just like the binomial regressions we saw earlier, we can use `margins::margins()` function to get the average marginal effects of covariates.

```{r }
pois_marginal_e <- margins(pois_lf, type = "response")
```
